{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the coverage for a given scenario\n",
    "A scenario defines the list of locations to consider for the coverage model. Scenarios are generated with notebook 5. Examples are: Mix, Subway, POis, Grid\n",
    "- 1.Mix scenario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pandas\n",
    "#! pip install numpy\n",
    "#! pip install matplotlib\n",
    "#! pip install yaml\n",
    "#! pip install scipy\n",
    "#! pip install seaborn\n",
    "#! pip install os\n",
    "#! pip install math\n",
    "#! pip install datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'geolife_data_path': '../../dataset', 'geolife_path': '../../dataset/geo_life_full.csv', 'geo_life_analysis_path': '../output/GeoLife_analysis/', 'out_path': '../output/coverage/', 'poi_path': '../output/coverage/MIX.csv', 'start_time': '2008-07-01 00:00:00', 'end_time': '2009-12-31 23:59:59', 'beijing_lat_min': 39.54, 'beijing_lat_max': 40.3, 'beijing_lon_min': 115.75, 'beijing_lon_max': 117.13, 'speed_filter': 500, 'stop_radius_factor': 0.1, 'minutes_for_a_stop': 60, 'spatial_radius_km': 0.1, 'extended_medoids': 100, 'cluster_radius_km': 0.2, 'min_cluster_stop_sample': 4, 'interpolation_distance': 200, 'n_cars': 0, 'n_bikes': 0, 'n_pedestrians': 100, 'n_trajectory_user': 50, 'beijing_center_lon': 116.3912757, 'beijing_center_lat': 39.906217, 'distance_from_center': 50000, 'sw_lon': 115.7657, 'sw_lat': 39.6332, 'ne_lon': 116.7435, 'ne_lat': 40.1411, 'gripd_stepsize': 6561.68, 'scales_values': [50], 'detour_radius': 800, 'coverage_start_period': '2009-05-01 00:00:00', 'coverage_end_period': '2009-05-31 00:00:00'}\n",
      "2009-05-01_2009-05-31\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import scipy.integrate as integrate\n",
    "from scipy.stats import expon\n",
    "import seaborn as sns\n",
    "import os \n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "with open(\"conf.yaml\") as f:\n",
    "    conf = yaml.load(f, Loader = yaml.FullLoader)\n",
    "\n",
    "out_path = conf[\"out_path\"]\n",
    "poi_path = conf[\"poi_path\"]\n",
    "geolife_data_path = conf[\"geolife_data_path\"]\n",
    "geolife_path = conf[\"geolife_path\"]\n",
    "scales_values = conf[\"scales_values\"]\n",
    "detour_radius = conf[\"detour_radius\"] \n",
    "\n",
    "beijing_lat_min = conf[\"beijing_lat_min\"]\n",
    "beijing_lat_max = conf[\"beijing_lat_max\"]\n",
    "beijing_lon_min = conf[\"beijing_lon_min\"]\n",
    "beijing_lon_max = conf[\"beijing_lon_max\"]\n",
    "\n",
    "#used to parse the conf scales\n",
    "def convert(s):\n",
    "    try:\n",
    "        return float(s)\n",
    "    except ValueError:\n",
    "        num, denom = s.split('/')\n",
    "        return float(num) / float(denom)\n",
    "\n",
    "scales = (list(map(convert, scales_values)))\n",
    "\n",
    "# Check and read the start end period period for computing the coverage. \n",
    "if \"coverage_start_period\" and \"coverage_end_period\" in conf:\n",
    "    coverage_start_period = conf[\"coverage_start_period\"]\n",
    "    coverage_end_period = conf[\"coverage_end_period\"]\n",
    "else:\n",
    "    # coverage period overlaps with the start and end time of the data set\n",
    "    coverage_start_period = conf[\"start_time\"]\n",
    "    coverage_end_period = conf[\"end_time\"]\n",
    "\n",
    "#start_time_obj = datetime.strptime(coverage_start_period, '%Y-%m-%d %H:%M:%S')\n",
    "start_time_obj = datetime.strptime(coverage_start_period, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "\n",
    "#end_time_obj = datetime.strptime(coverage_end_period, '%Y-%m-%d %H:%M:%S')\n",
    "end_time_obj = datetime.strptime(coverage_end_period, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "#a = str(start_time_obj.year)+\"-\"+str(start_time_obj.month)+\"-\"+str(start_time_obj.day)\n",
    "#b = str(end_time_obj.year)+\"-\"+str(end_time_obj.month)+\"-\"+str(end_time_obj.day)\n",
    "#suffix = a+\"_\"+b\n",
    "suffix = start_time_obj+\"_\"+end_time_obj\n",
    "\n",
    "\n",
    "print(conf)\n",
    "print(suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoLife augmented loaded with period: 2009-05-01_2009-05-31\n",
      "982306\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "cols = [\"lat\", \"lon\", \"uid\", \"tid\",\"date_time\"]\n",
    "dataset = pd.read_csv(geolife_path, usecols=cols, parse_dates = True)\n",
    "\n",
    "#restricting to the coverage period, if specified\n",
    "if \"coverage_start_period\" and \"coverage_end_period\" in conf:\n",
    "    dataset = dataset[(dataset.date_time >= coverage_start_period) & (dataset.date_time <= coverage_end_period)]\n",
    "    \n",
    "#restricting mobility to beijing area\n",
    "dataset = dataset[(dataset['lat'].between(beijing_lat_min, beijing_lat_max )) & (dataset['lon'].between(beijing_lon_min, beijing_lon_max))]\n",
    "\n",
    "print(\"GeoLife augmented loaded with period:\",suffix)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define a basic Haversine distance formula\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1 \n",
    "    dlon = lon2 - lon1 \n",
    "    a = (np.sin(dlat/2)**2) + (np.cos(lat1)) * (np.cos(lat2)) * (np.sin(dlon/2)**2)\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    #c = np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    #total_meters = METERS * c\n",
    "    r = 6371000 #radiu * 1000 to return meters\n",
    "    return (c * r)\n",
    "\n",
    "# Coverage functions\n",
    "def coverage(locations_id, distances, scale):\n",
    "    coverage = pd.DataFrame()\n",
    "    coverage[\"id_location\"] = locations_id\n",
    "    coverage[\"probability\"] = calculate_coverage(locations_id, distances, scale)\n",
    "    return coverage\n",
    "\n",
    "def calculate_coverage(locations_id, distances, scale):\n",
    "    coverages = []\n",
    "    \n",
    "    #initialize coverage inner function\n",
    "    inner = lambda x: expon.pdf(x,scale=scale)\n",
    "    print(\"Calculating coverage with scale:\",scale)\n",
    "    counter = 0\n",
    "    for loc in locations_id:\n",
    "        #print(\"Processing location,count, tot: \",loc,counter,len(locations_id))\n",
    "        counter+=1\n",
    "        dists_location = distances[distances.id_location == loc]\n",
    "        #if the location has no points whatsoever, assign 0 to the coverage value and continue\n",
    "        if (len(dists_location) == 0):\n",
    "            coverages.append(0)\n",
    "            #print(\"no points for location: \",loc)\n",
    "            continue\n",
    "        \n",
    "        # the list of ids of the users with some points close to the location\n",
    "        uids = dists_location.uid.unique()\n",
    "        # users\n",
    "        w = []\n",
    "        for user in uids:\n",
    "            #print(user)\n",
    "            dists_location_user = dists_location[(dists_location.uid == user)]\n",
    "            #print(\"user:\",dists_location_user)\n",
    "            if(len(dists_location_user) == 0):\n",
    "                #print(\"WARNING: no user's points for: \",loc)\n",
    "                continue\n",
    "            # Trajectories\n",
    "            tids = dists_location_user.tid.unique()\n",
    "            k = []\n",
    "            for traj in tids:\n",
    "                dists_location_user_traj = dists_location_user[(dists_location_user.tid == traj)]\n",
    "                #print(\"traj:\",dists_location_user_traj)\n",
    "                if(len(dists_location_user_traj) == 0):\n",
    "                    #print(\"WARNIN: no user's traj for: \",loc)\n",
    "                    continue\n",
    "                # min distance\n",
    "                min_distance  = dists_location_user_traj[\"distance\"].iloc[0]\n",
    "                #y = integrate.quad(inner, min_distance, np.inf)[0]\n",
    "                ## Truncate of the Exponential Distribution\n",
    "                y = integrate.quad(inner, min_distance, np.inf)[0]/(1-math.exp(-scale*detour_radius))\n",
    "\n",
    "                #print(\"min_distance\",min_distance)\n",
    "                #print(\"1-pdf\",y)\n",
    "                k.append(1-y)\n",
    "                #print(\"cumulato\",k)\n",
    "            m=1-np.prod(k)\n",
    "            w.append(1-m)\n",
    "        coverages.append(1-np.prod(w))\n",
    "\n",
    "    #import pdb; pdb.set_trace()                    \n",
    "    return coverages\n",
    "\n",
    "#calculates coverages on multiple scale values and serializes them on disk\n",
    "def coverage_multiple_scales(scales, locations_id, distances,scenario,locations):\n",
    "    for scale in scales:\n",
    "        # copute the coverage prob\n",
    "        coverages = coverage(locations_id, distances, scale)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        #merge with df_t dataframe with location positions on \"location\" id then serialize on disk\n",
    "        merged = locations.merge(coverages, on=\"id_location\")\n",
    "        name =  \"coverages_\" +suffix+\".csv\"\n",
    "        scale_dest = str(float(scale))\n",
    "        merged.to_csv(os.path.join(out_path,\"scenario\",scenario,scale_dest, name))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mix scenario\n",
    "- Load the scenario with locations selected according to MIX scenario: POIs + Random points\n",
    "- Filter locations according to the detour_radius\n",
    "- Extract the IDS of the locations\n",
    "- Invoke the coverage_multiple_scales function for generating the coverage probability  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locations found  1954\n",
      "There are 193 locations that have no points in detour radius' range\n"
     ]
    }
   ],
   "source": [
    "locations = pd.read_csv(poi_path)\n",
    "print(\"Locations found \", len(locations))\n",
    "\n",
    "# filter POIs according to the Beijing area\n",
    "locations = locations[(locations['lat'].between(beijing_lat_min, beijing_lat_max )) & (locations['lon'].between(beijing_lon_min, beijing_lon_max))]\n",
    "\n",
    "\n",
    "# list of distances\n",
    "dists = []\n",
    "count = 0\n",
    "counter = 0\n",
    "\n",
    "for index, location in locations.iterrows():\n",
    "    #print(\"Processing:\",counter,len(locations))\n",
    "    counter+=1\n",
    "    #print(\"location:\",index)\n",
    "   \n",
    "    # distance between each loacation and ALL the dataset locations (input is: lat, lon, np.array(), np.array())\n",
    " #   import pdb; pdb.set_trace()                    \n",
    "    d_i_h = pd.DataFrame()\n",
    "   # d_i_h[\"distance\"] = haversine(location.lat, location.lon, dataset[\"lat\"].values, dataset[\"lon\"].values)\n",
    "    d_i_h[\"uid\"] = dataset[\"uid\"]\n",
    "    d_i_h[\"tid\"] = dataset[\"tid\"]\n",
    "    d_i_h[\"lon\"] = dataset[\"lon\"]\n",
    "    d_i_h[\"lat\"] = dataset[\"lat\"]\n",
    "    d_i_h[\"distance\"] = haversine(location.lat, location.lon, dataset.lat, dataset.lon)\n",
    "   \n",
    "   # d_i_h[\"index\"] = range(1, len(d_i_h) + 1)\n",
    "   # d_i_h[\"date_time\"] = dataset[\"date_time\"] \n",
    "   # print(poi_path)\n",
    "   # print(location)\n",
    "   # pd.set_option('display.max_rows', None)\n",
    "    #print(\"before sorting \\n\",d_i_h[(d_i_h.tid == 18949) & (d_i_h.index==6018)])\n",
    "\n",
    "    \n",
    "    # filter distances w.r.t detour_radius\n",
    "    d_i_h = d_i_h[d_i_h.distance <= detour_radius]\n",
    "  \n",
    "    if (len(d_i_h) > 0):\n",
    "        d_i_h = d_i_h.groupby(by=\"tid\").min()\n",
    "        d_i_h.reset_index(inplace=True)\n",
    "        d_i_h[\"id_location\"] = location.id_location\n",
    "        # TODO: aggiungere la posizione della distanza minima\n",
    "        dists.append(d_i_h)\n",
    "    else:\n",
    "        #d_i_h = pd.DataFrame({\"location\":location.id_location, \"uid\":np.nan, \"distance\":np.nan}, index=[0])\n",
    "        count += 1\n",
    "     #   d_i_h = d_i_h.iloc[0:0]  \n",
    "    \n",
    "print(\"There are {:d} locations that have no points in detour radius' range\".format(count)) \n",
    "subway_dists = pd.concat(dists)\n",
    "subway_dists.to_csv(out_path+\"subway_dists_dr_\"+str(detour_radius)+\"_\"+suffix+\".csv\")\n",
    "#print(d_i_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ocations of interests:  1954\n",
      "Reading:  ../output/coverage/subway_dists_dr_800_2009-05-01_2009-05-31.csv\n",
      "Locations of interests with points in detour radius range:  1722\n",
      "         tid  uid         lon        lat    distance   id_location\n",
      "0       5731   41  115.973697  39.639435  483.909737  2.391265e+09\n",
      "1       5732   41  115.961088  39.638997  261.238860  2.391265e+09\n",
      "2       8863   84  115.928728  39.682084  308.118320  4.643050e+09\n",
      "3       5731   41  116.071088  39.716338  289.853397  5.315328e+09\n",
      "4       5732   41  116.082860  39.716768  665.720593  5.315328e+09\n",
      "...      ...  ...         ...        ...         ...           ...\n",
      "53063  16977  155  116.332175  39.974827   86.243664  2.311466e+04\n",
      "53064  16978  155  116.330922  39.974257  120.912107  2.311466e+04\n",
      "53065  16979  155  116.329353  39.974843  270.243668  2.311466e+04\n",
      "53066  18651  180  116.330222  39.976189  404.207409  2.311466e+04\n",
      "53067  18652  180  116.330479  39.976229  376.226066  2.311466e+04\n",
      "\n",
      "[53068 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# to be executed after distance processing\n",
    "# list of locations\n",
    "locations = pd.read_csv(poi_path)\n",
    "locations_ids = locations.id_location.unique()\n",
    "print(\"Number of ocations of interests: \", len(locations_ids))\n",
    "\n",
    "# the distances from the locations\n",
    "dist_name = out_path+\"subway_dists_dr_\"+str(detour_radius)+\"_\"+suffix+\".csv\"\n",
    "print(\"Reading: \",dist_name)\n",
    "subway_dists = pd.read_csv(dist_name)\n",
    "subway_dists.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "                      \n",
    "print(\"Locations of interests with points in detour radius range: \", len(subway_dists.id_location.unique()))\n",
    "print(subway_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute the coverage given the:\n",
    "- scales\n",
    "- ids of the locations\n",
    "- distances\n",
    "- name of the scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating coverage with scale: 50.0\n"
     ]
    }
   ],
   "source": [
    "scenario_name = \"mix\" #used to get a meeaningful name for out .csv\n",
    "coverage_multiple_scales(scales, locations_ids, subway_dists, scenario_name,locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
