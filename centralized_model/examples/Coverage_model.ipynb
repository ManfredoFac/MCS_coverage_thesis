{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "sigma = 0.44\n",
    "mu = 0\n",
    "bins = np.linspace(-10, 10, 10)\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2) ), linewidth=2, color='r')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(bins, norm.pdf(bins),'r-', lw=2, alpha=0.6, label='norm pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "s = np.random.normal(mu, sigma, 1000)\n",
    "count, bins, ignored = plt.hist(s, 30, density=True)\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n",
    "               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n",
    "         linewidth=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso distribuzione esponeniale per modellerare densità di probabilità della v.a. $D$\n",
    "- la densità di probabilità di coverage della locazione $l_i$ in funzione della valori della v.a. $D, d \\in \\mathbb{R^+}$ è data data da una funzione esponenziale di media $E[D] = \\frac{1}{\\lambda}$\n",
    "- i valori assunti dalla v.a. $D$ sono generati con una distribuzione random e com una distribuzione esponenziale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import expon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#numero di campioni da generare\n",
    "n = 10000\n",
    "# scale = 1/\\lambda , quindi scale è la media della distribuzione\n",
    "scale = 5\n",
    "\n",
    "#genero dei campioni delle distanze minime dalle locazioni di interesse con una distribuzione random\n",
    "distances = np.random.randint(10,50,n)\n",
    "close_distances = np.random.randint(0,10,n)\n",
    "print(np.mean(distances))\n",
    "#var = scale^2 (1/\\lambda^2)\n",
    "print(\"var\",np.var(distances))\n",
    "\n",
    "#calcolo la pdf per ogni valore delle distanze generata\n",
    "densities = expon.pdf(distances,scale=scale)\n",
    "plt.plot(distances,densities,'*')\n",
    "plt.xlabel(\"minime distanze da $l_i$ random\")\n",
    "plt.axvline(x=50,color=\"r\")\n",
    "plt.axvline(x=10,color=\"r\")\n",
    "plt.savefig(\"../output/Exp_model.png\", dpi  = 300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import expon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#numero di campioni da generare\n",
    "n = 100\n",
    "# scale = 1/\\lambda , quindi scale è la media della distribuzione\n",
    "scale = 5\n",
    "\n",
    "#genero dei campioni delle distanze minime dalle locazioni di interesse con una distribuzione esponenziale di media scale\n",
    "distances = np.random.exponential(scale,(n,1))     \n",
    "print(np.mean(distances))\n",
    "#var = scale^2 (1/\\lambda^2)\n",
    "print(\"var\",np.var(distances))\n",
    "\n",
    "#calcolo la pdf per ogni valore delle distanze generata\n",
    "densities = expon.pdf(distances,scale=scale)\n",
    "plt.plot(distances,densities,'*')\n",
    "plt.xlabel(\"samples $d \\in D^h$ \")\n",
    "plt.ylabel(\"Exponential PDF with scale = 1/$\\lambda$ = 5\")\n",
    "plt.savefig(\"../output/Exp_model_exp_samples.png\", dpi  = 300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expon.ppf(0.5, loc=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andamento della PDF della exp e della probabilità (integrale pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import expon\n",
    "import seaborn as sns\n",
    "from pylab import *\n",
    "\n",
    "plt.rc('xtick', labelsize=30) \n",
    "plt.rc('ytick', labelsize=30) \n",
    "\n",
    "sns.set(style=\"ticks\",font_scale=3)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "#ax.plot(r, rv.pdf(r), 'k-', lw=2, label='frozen pdf')\n",
    "#ax.hist(r, histtype='stepfilled')\n",
    "\n",
    "#x = np.linspace(expon.ppf(0.01),expon.ppf(0.99), 100)\n",
    "x = np.linspace(1,1200, 1000)\n",
    "\n",
    "scale = 800\n",
    "plt.plot(x, expon.pdf(x,scale=scale),'g-', lw=5, alpha=0.9, label='$1/\\lambda$ = '+str(scale))\n",
    "plt.axvline(x=scale,linestyle=\"dashed\",color=\"black\")\n",
    "plt.xlabel(\"distance\")\n",
    "#plt.plot(x, expon.pdf(x,scale=50),'b', lw=5, alpha=0.9, label='scale = $50$')\n",
    "#plt.axvline(x=50,linestyle=\"dashed\",color=\"black\")\n",
    "#plt.plot(x, expon.pdf(x,scale=500),'g', lw=5, alpha=0.9, label='scale = $500$')\n",
    "#plt.axvline(x=500,linestyle=\"dashed\",color=\"black\")\n",
    "plt.legend(loc=1)\n",
    "plt.tight_layout()\n",
    "\n",
    "savefig(\"exp_scale_\"+str(scale)+\".png\", dpi  = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate as integrate\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "def proba(color,scale=1):\n",
    "    delta = 0.0001\n",
    "\n",
    "    inner = lambda x: expon.pdf(x,scale=scale)\n",
    "    integral = lambda x: integrate.quad(inner, x-delta, x+delta)[0]\n",
    "    vec_integral = np.vectorize(integral)\n",
    "\n",
    "    #x = np.linspace(1,500, 100)\n",
    "    x = np.array([65.085831,48.431312,45.991198])\n",
    "    x1 = np.array([427.473465,396.073703,374.619374])\n",
    "    #x2 = np.array([20,10,9,8,7,6])\n",
    "    \n",
    "    x2 = np.linspace(1,50,10)\n",
    "    y = vec_integral(x)\n",
    "    y1 = vec_integral(x1)\n",
    "    y2 = vec_integral(x2)\n",
    "    \n",
    "    values = np.linspace(1,100,100)\n",
    "    \n",
    "    \n",
    "    print(y,y1,y2)\n",
    "    #u1= np.sum(y)-np.prod(y)\n",
    "    u1= np.sum(y)\n",
    "    u2=np.sum(y1)\n",
    "    #import pdb; pdb.set_trace()\n",
    "    u3=np.sum(y2)-np.prod(y2)\n",
    "    print(\"coverage:\",u1,u2,u3)\n",
    "    \n",
    "\n",
    "    #plt.plot(x,y,\"b\",)\n",
    "    #plt.plot(x1,y1,\"r\")\n",
    "    #plt.plot(x2,y2,\"g\")\n",
    "    plt.plot(value,y2,\"g\")\n",
    "    plt.axvline(x=scale,color=\"r\",alpha=0.5)\n",
    "    return np.prod(y)\n",
    "\n",
    "a = proba(\"b\",50)\n",
    "#b = proba(\"*-b\",100)\n",
    "#c = proba(\"*-r\",200)\n",
    "#print(np.array([a,b,c]))\n",
    "#print(np.sort(np.array([a,b,c])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valus = np.linspace(1,800,500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate as integrate\n",
    "import numpy as np\n",
    "from scipy.stats import expon\n",
    "\n",
    "# calcolo intervalli di integrazione senza sovrapposizione\n",
    "def prob_coverage(points,scale):\n",
    "    delta = 1\n",
    "    inner = lambda x: expon.pdf(x,scale=scale)\n",
    "    #integral = lambda x: integrate.quad(inner, x, x+delta)[0]\n",
    "    #vec_integral = np.vectorize(integral)\n",
    "\n",
    "    upper_d = 0\n",
    "    probs = []\n",
    "    for p in points:\n",
    "        #import pdb; pdb.set_trace()\n",
    "        orig = p\n",
    "        if p < upper_d:\n",
    "            p = upper_d\n",
    "        upper_d = orig+delta\n",
    "        #print(p,upper_d)\n",
    "        prob = integrate.quad(inner, p, upper_d)[0]\n",
    "        probs.append(prob)\n",
    "    return np.sum(probs)\n",
    "\n",
    "scale = 20\n",
    "x1 = np.sort(np.array([65.085831,48.431312,45.991198]))\n",
    "x2 = np.sort(np.array([427.473465,396.073703,374.619374]))\n",
    "x3 = np.sort(np.array([20,30,9,8,7,6]))\n",
    "x4 = np.linspace(1,30,100)\n",
    "a = np.linspace(1,1000,1000)\n",
    "\n",
    "\n",
    "\n",
    "#print(x4)\n",
    "p1 = (prob_coverage(x1,scale))\n",
    "p2 = (prob_coverage(x2,scale))\n",
    "p3 = (prob_coverage(x3,scale))\n",
    "p4 = (prob_coverage(x4,scale))\n",
    "p5 = (prob_coverage(a,scale))\n",
    "probs = np.array([p1,p2,p3,p4,p5])\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(1,high=2,size=10000)\n",
    "print(a)\n",
    "delta = 1\n",
    "inner = lambda x: expon.pdf(x,scale=1)\n",
    "integral = lambda x: integrate.quad(inner, x, x+0.001)[0]\n",
    "vec_integral = np.vectorize(integral)\n",
    "np.sum(vec_integral(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs)\n",
    "np.quantile(probs,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.full(3, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementazione della inclusion-exclusion https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle#In_probability\n",
    "from itertools import combinations\n",
    "probs = np.full(1, 0.5)\n",
    "\n",
    "z = len(probs)\n",
    "users = np.arange(0,len(probs),1)\n",
    "print(users)\n",
    "sign = -1\n",
    "result = []\n",
    "for i  in range(2,z+1):\n",
    "    print(\"----------combinations: \",i)\n",
    "    combs = combinations(users, i)\n",
    "    somme = 0\n",
    "    for comb in list(combs):\n",
    "        #print(\"combination\", comb)\n",
    "        t = []\n",
    "        for user in list(zip(comb)):\n",
    "            the_index = user[0]\n",
    "            t.append(probs[the_index])\n",
    "            #print(probs[the_index])\n",
    "        somme+=np.prod(t)\n",
    "    print(\"Somme,\",somme)\n",
    "    result.append(sign*somme)\n",
    "    sign*=-1\n",
    "\n",
    "print(result)\n",
    "np.sum(probs) + np.sum(result)\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modello aggiornato 16-02-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.integrate as integrate\n",
    "from scipy.stats import expon\n",
    "\n",
    "def cov(a):\n",
    "    inner = lambda x: expon.pdf(x,scale=10)\n",
    "    integral = lambda y: 1-integrate.quad(inner, y, np.inf)[0]\n",
    "    vec_integral = np.vectorize(integral)\n",
    "    a = vec_integral(a)\n",
    "    a = 1-np.prod(a)\n",
    "    return a\n",
    "    \n",
    "a = np.array([10,11,12,14])\n",
    "b = np.array([20,21,22,23])\n",
    "print(cov(a))\n",
    "print(cov(b))\n",
    "\n",
    "a = np.array([0.8,0.8,0.8])\n",
    "#100 punti a 100 metric > 2 punti a 10 metri\n",
    "b = np.array([0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,])\n",
    "print(1-np.prod(1-a))\n",
    "print(1-np.prod(1-b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expon.cdf(30,scale=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.2*0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (1-0.2)*(1-0.3)\n",
    "1-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.2*0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1-(1-0.2))*(1-(1-0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (1-0.2)*(1-0.3)\n",
    "1-(1-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (1-(1-0.2))*(1-(1-0.3))*(1-(1-0.4))*(1-(1-0.9))\n",
    "b = 0.2*0.3*0.4*0.9\n",
    "print(a,b)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.default_rng().laplace(loc=13.5,scale=0.5,size=1000)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.hist(r, histtype='stepfilled',density = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sort(np.array([3,2,5,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.01\n",
    "scale = 10\n",
    "inner = lambda x: expon.pdf(x,scale=scale)\n",
    "integral = lambda x: integrate.quad(inner, x-delta, x+delta)[0]\n",
    "vec_integral = np.vectorize(integral)\n",
    "print(vec_integral(48))\n",
    "print(vec_integral(97))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random(10)\n",
    "print(a)\n",
    "1-a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esempio per fitting di una distribuzione usando le minime distanze dalle locazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import statsmodels as sm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (16.0, 12.0)\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# Create models from data\n",
    "def best_fit_distribution(data, bins=200, ax=None):\n",
    "    \"\"\"Model data by finding best fit distribution to data\"\"\"\n",
    "    # Get histogram of original data\n",
    "    y, x = np.histogram(data, bins=bins, density=True)\n",
    "    x = (x + np.roll(x, -1))[:-1] / 2.0\n",
    "\n",
    "    # Distributions to check\n",
    "    DISTRIBUTIONS = [        \n",
    "        st.alpha,st.anglit,st.arcsine,st.beta,st.betaprime,st.bradford,st.burr,st.cauchy,st.chi,st.chi2,st.cosine,\n",
    "        st.dgamma,st.dweibull,st.erlang,st.expon,st.exponnorm,st.exponweib,st.exponpow,st.f,st.fatiguelife,st.fisk,\n",
    "        st.foldcauchy,st.foldnorm,st.frechet_r,st.frechet_l,st.genlogistic,st.genpareto,st.gennorm,st.genexpon,\n",
    "        st.genextreme,st.gausshyper,st.gamma,st.gengamma,st.genhalflogistic,st.gilbrat,st.gompertz,st.gumbel_r,\n",
    "        st.gumbel_l,st.halfcauchy,st.halflogistic,st.halfnorm,st.halfgennorm,st.hypsecant,st.invgamma,st.invgauss,\n",
    "        st.invweibull,st.johnsonsb,st.johnsonsu,st.ksone,st.kstwobign,st.laplace,st.levy,st.levy_l,st.levy_stable,\n",
    "        st.logistic,st.loggamma,st.loglaplace,st.lognorm,st.lomax,st.maxwell,st.mielke,st.nakagami,st.ncx2,st.ncf,\n",
    "        st.nct,st.norm,st.pareto,st.pearson3,st.powerlaw,st.powerlognorm,st.powernorm,st.rdist,st.reciprocal,\n",
    "        st.rayleigh,st.rice,st.recipinvgauss,st.semicircular,st.t,st.triang,st.truncexpon,st.truncnorm,st.tukeylambda,\n",
    "        st.uniform,st.vonmises,st.vonmises_line,st.wald,st.weibull_min,st.weibull_max,st.wrapcauchy,st.kde\n",
    "    ]\n",
    "    \n",
    "    DISTRIBUTIONS = [        \n",
    "        st.alpha,st.anglit,st.arcsine,st.beta,st.betaprime,st.bradford,st.burr,st.cauchy,st.chi,st.chi2,st.cosine,\n",
    "        st.dgamma,st.dweibull,st.erlang,st.expon,st.exponnorm,st.exponweib,st.exponpow,st.f,st.fatiguelife,st.fisk,\n",
    "        st.norm, st.pareto,st.kde\n",
    "    ]\n",
    "\n",
    "    # Best holders\n",
    "    best_distribution = st.norm\n",
    "    best_params = (0.0, 1.0)\n",
    "    best_sse = np.inf\n",
    "\n",
    "    # Estimate distribution parameters from data\n",
    "    for distribution in DISTRIBUTIONS:\n",
    "        print(\"trying: \",distribution)\n",
    "        # Try to fit the distribution\n",
    "        try:\n",
    "            # Ignore warnings from data that can't be fit\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore')\n",
    "\n",
    "                # fit dist to data\n",
    "                params = distribution.fit(data)\n",
    "\n",
    "                # Separate parts of parameters\n",
    "                arg = params[:-2]\n",
    "                loc = params[-2]\n",
    "                scale = params[-1]\n",
    "\n",
    "                # Calculate fitted PDF and error with fit in distribution\n",
    "                pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)\n",
    "                # compute the error\n",
    "                sse = np.sum(np.power(y - pdf, 2.0))\n",
    "\n",
    "                # if axis  is not null, plot this pdf as well\n",
    "                try:\n",
    "                    if ax:\n",
    "                        pd.Series(pdf, x).plot(ax=ax)\n",
    "                    end\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                # identify if this distribution is better\n",
    "                if best_sse > sse > 0:\n",
    "                    best_distribution = distribution\n",
    "                    best_params = params\n",
    "                    best_sse = sse\n",
    "\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return (best_distribution.name, best_params)\n",
    "\n",
    "def make_pdf(dist, params, size=10000):\n",
    "    \"\"\"Generate distributions's Probability Distribution Function \"\"\"\n",
    "\n",
    "    # Separate parts of parameters\n",
    "    arg = params[:-2]\n",
    "    loc = params[-2]\n",
    "    scale = params[-1]\n",
    "\n",
    "    # Get sane start and end points of distribution\n",
    "    start = dist.ppf(0.01, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.01, loc=loc, scale=scale)\n",
    "    end = dist.ppf(0.99, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.99, loc=loc, scale=scale)\n",
    "\n",
    "    # Build PDF and turn into pandas Series\n",
    "    x = np.linspace(start, end, size)\n",
    "    y = dist.pdf(x, loc=loc, scale=scale, *arg)\n",
    "    pdf = pd.Series(y, x)\n",
    "\n",
    "    return pdf\n",
    "\n",
    "fig, (ax,ax2) = plt.subplots(2, 1, figsize=(12,8))\n",
    "\n",
    "n = 99000\n",
    "# Load data: randomly\n",
    "data = np.random.randint(1,100,n)\n",
    "\n",
    "# Load data: following a normal distribution of mu and signa \n",
    "mu, sigma = 5, 1 # mean and standard deviation\n",
    "data = np.random.normal(mu, sigma, n)\n",
    "\n",
    "\n",
    "# load data: following an exponential distribtion of mean scale\n",
    "scale = 5\n",
    "#genero dei campioni delle distanze minime dalle locazioni di interesse con una distribuzione esponenziale di media scale\n",
    "#data = np.random.exponential(scale,(n,1))  \n",
    "\n",
    "\n",
    "# plot hist of the data\n",
    "bins = 200\n",
    "ax.hist(data,bins=bins,alpha=0.4)\n",
    "\n",
    "# Find best fit distribution and plot in the same ax\n",
    "# best distribution minimizes the  Sum of Square Error (SSE) \n",
    "best_fit_name, best_fit_params = best_fit_distribution(data, bins, ax)\n",
    "best_dist = getattr(st, best_fit_name)\n",
    "\n",
    "# Update plots\n",
    "#ax.set_ylim(dataYLim)\n",
    "\n",
    "# Make PDF with best distributiobn and best params\n",
    "pdf = make_pdf(best_dist, best_fit_params)\n",
    "\n",
    "\n",
    "# Display\n",
    "\n",
    "pdf.plot(lw=2, label='PDF', legend=True,ax = ax2)\n",
    "\n",
    "\n",
    "param_names = (best_dist.shapes + ', loc, scale').split(', ') if best_dist.shapes else ['loc', 'scale']\n",
    "param_str = ', '.join(['{}={:0.2f}'.format(k,v) for k,v in zip(param_names, best_fit_params)])\n",
    "dist_str = '{}({})'.format(best_fit_name, param_str)\n",
    "ax2.legend([dist_str,\"observations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:/Users/michele/Documents/datasets/mobility/Geolife_Trajectories_1.3/Data/geolife_filtered.csv\")\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"user\",len(data.uid.unique()))\n",
    "print(\"tid\",len(data.tid.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(by=\"uid\").max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[data.uid==0][\"tid\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
