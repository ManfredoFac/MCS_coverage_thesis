{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import current\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "       \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "       This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "       and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "       does not exist.\n",
    "       \"\"\"\n",
    "       name = fullname.rsplit('.', 1)[-1]\n",
    "       if not path:\n",
    "           path = ['']\n",
    "       for d in path:\n",
    "           nb_path = os.path.join(d, name + \".ipynb\")\n",
    "           if os.path.isfile(nb_path):\n",
    "               return nb_path\n",
    "           # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "           nb_path = nb_path.replace(\"_\", \" \")\n",
    "           if os.path.isfile(nb_path):\n",
    "               return nb_path\n",
    "\n",
    "\n",
    "class NotebookLoader(object):\n",
    "       \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "       def __init__(self, path=None):\n",
    "           self.shell = InteractiveShell.instance()\n",
    "           self.path = path\n",
    "\n",
    "       def load_module(self, fullname):\n",
    "           \"\"\"import a notebook as a module\"\"\"\n",
    "           path = find_notebook(fullname, self.path)\n",
    "\n",
    "           print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "           # load the notebook object\n",
    "           with io.open(path, 'r', encoding='utf-8') as f:\n",
    "               nb = current.read(f, 'json')\n",
    "\n",
    "\n",
    "           # create the module and add it to sys.modules\n",
    "           # if name in sys.modules:\n",
    "           #    return sys.modules[name]\n",
    "           mod = types.ModuleType(fullname)\n",
    "           mod.__file__ = path\n",
    "           mod.__loader__ = self\n",
    "           mod.__dict__['get_ipython'] = get_ipython\n",
    "           sys.modules[fullname] = mod\n",
    "\n",
    "           # extra work to ensure that magics that would affect the user_ns\n",
    "           # actually affect the notebook module's ns\n",
    "           save_user_ns = self.shell.user_ns\n",
    "           self.shell.user_ns = mod.__dict__\n",
    "\n",
    "           try:\n",
    "               for cell in nb.worksheets[0].cells:\n",
    "                   if cell.cell_type == 'code' and cell.language == 'python':\n",
    "                       # transform the input to executable Python\n",
    "                       code = self.shell.input_transformer_manager.transform_cell(cell.input)\n",
    "                       # run the code in themodule\n",
    "                       exec(code, mod.__dict__)\n",
    "           finally:\n",
    "               self.shell.user_ns = save_user_ns\n",
    "           return mod\n",
    "\n",
    "\n",
    "class NotebookFinder(object):\n",
    "       \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "       def __init__(self):\n",
    "           self.loaders = {}\n",
    "\n",
    "       def find_module(self, fullname, path=None):\n",
    "           nb_path = find_notebook(fullname, path)\n",
    "           if not nb_path:\n",
    "               return\n",
    "\n",
    "           key = path\n",
    "           if path:\n",
    "               # lists aren't hashable\n",
    "               key = os.path.sep.join(path)\n",
    "\n",
    "           if key not in self.loaders:\n",
    "               self.loaders[key] = NotebookLoader(path)\n",
    "           return self.loaders[key]\n",
    "\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "\n",
    "from KStationPositioning import KStationPositioning, newDfCoverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UAV Deployment\n",
    "- 1.Funzioni generali\n",
    "- 2.StationPositioning\n",
    "- 3.Random\n",
    "- 4.DBSCAN\n",
    "- 5.k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shapely\n",
    "#import pyproj\n",
    "#from pyproj import CRS\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas\n",
    "#from planar import BoundingBox\n",
    "#from geopy import distance \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import cm\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.collections import PatchCollection\n",
    "from scipy.interpolate import griddata\n",
    "import numpy as np\n",
    "#import planar\n",
    "import yaml\n",
    "#from geographiclib.geodesic import Geodesic\n",
    "import cartopy.geodesic as GD\n",
    "import math\n",
    "from shapely.geometry import shape\n",
    "from shapely.prepared import prep\n",
    "import os\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "with open('../conf.yaml') as f:\n",
    "    conf = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "pathGeneral = conf[\"pathGeneral\"]\n",
    "\n",
    "#### Algorithm settings ####\n",
    "\n",
    "# UAV radius\n",
    "Dr = conf[\"uav_radius\"]\n",
    "pathBox = conf[\"pathBox\"]\n",
    "Cr = conf[\"tau\"]\n",
    "R0_iterations = conf[\"R0_iterations\"]\n",
    "min_samples = conf[\"min_samples\"]\n",
    "\n",
    "# Check and read the start end period period for computing the coverage. \n",
    "if \"coverage_start_period\" and \"coverage_end_period\" in conf:\n",
    "    coverage_start_period = conf[\"coverage_start_period\"]\n",
    "    coverage_end_period = conf[\"coverage_end_period\"]\n",
    "else:\n",
    "    # coverage period overlaps with the start and end time of the data set\n",
    "    coverage_start_period = conf[\"start_time\"]\n",
    "    coverage_end_period = conf[\"end_time\"]\n",
    "\n",
    "start_time_obj = datetime.strptime(coverage_start_period, '%Y-%m-%d %H:%M:%S')\n",
    "end_time_obj = datetime.strptime(coverage_end_period, '%Y-%m-%d %H:%M:%S')\n",
    "a = str(start_time_obj.year)+\"_\"+str(start_time_obj.month)+\"_\"+str(start_time_obj.day)\n",
    "b = str(end_time_obj.year)+\"_\"+str(end_time_obj.month)+\"_\"+str(end_time_obj.day)\n",
    "# build suffix for the output file namnes\n",
    "suffix = a+\"-\"+b\n",
    "\n",
    "#Load box for experiments: high and border\n",
    "Box = geopandas.GeoDataFrame(geopandas.read_file(pathBox),crs=\"EPSG:4326\")\n",
    "Box=Box.set_index('index')\n",
    "\n",
    "boxHigh=Box.at[\"boxHigh\",'geometry']\n",
    "boxBorder=Box.at[\"boxBorder\",'geometry']\n",
    "\n",
    "# build the box's paths\n",
    "pathDFHigh = conf[\"pathDFHigh\"]+suffix+\"/\"\n",
    "pathDFBorder = conf [\"pathDFBorder\"]+suffix+\"/\"\n",
    "\n",
    "# Load number of UAV stations to deploy\n",
    "uav_stations = conf[\"uav_stations\"]\n",
    "\n",
    "\n",
    "\n",
    "# Load the un-admissible areas\n",
    "unadmissible_values = conf[\"unadmissible_values\"]\n",
    "unadmissible_areas = {}\n",
    "for value in unadmissible_values:\n",
    "    area_path = os.path.join(pathGeneral,\"unadmissible_area_\"+str(value)+\".shp\")\n",
    "    area = geopandas.GeoDataFrame(geopandas.read_file(area_path),crs = \"EPSG:4326\")\n",
    "    unadmissible_areas[str(value)] = area['geometry']\n",
    "\n",
    "#areeProibite = geopandas.GeoDataFrame(geopandas.read_file(pathAreeProibite),crs = \"EPSG:4326\")\n",
    "#areeProibite=areeProibite['geometry']\n",
    "print(\"----Loaded Settings----\")\n",
    "print(\"Dr: \",Dr)\n",
    "print(\"Tau:\",Cr)\n",
    "print(\"Period selected:\",suffix)\n",
    "print(\"Un-admissible area: \",unadmissible_areas.keys())\n",
    "print(\"Number of stations: \",uav_stations)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Funzioni generali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creazione mappa calore con/senza le aree proibite (in base al flag areeProibite)\n",
    "#path = path all'interno del quale viene salvata la heatmap\n",
    "#box = area a cui si riferiscono i punti in df\n",
    "#df = geodataframe\n",
    "\n",
    "def coverage_heatmap (df,unadmissible_area,box,path):\n",
    "    boundsBox = box.bounds\n",
    "    minlat=boundsBox[1]\n",
    "    maxlat=boundsBox[3]\n",
    "    minlon=boundsBox[0]\n",
    "    maxlon=boundsBox[2]\n",
    "    X = df.geometry.x.to_numpy()\n",
    "    Y = df.geometry.y.to_numpy()\n",
    "    Z = df.probability.to_numpy()\n",
    "    xi = np.linspace(minlon,maxlon,30)\n",
    "    yi = np.linspace(minlat,maxlat,30)\n",
    "    xi, yi = np.meshgrid(xi, yi)\n",
    "    x, y, z = df.geometry.x, df.geometry.y , df.probability\n",
    "    points = np.vstack((x,y)).T \n",
    "    # Z Ã¨ una matrice con i valori x-y\n",
    "    zi = griddata(points, Z, (xi,yi), method='linear',fill_value=0)\n",
    "    \n",
    "    # setting del plot\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figwidth(15)\n",
    "    fig.set_figheight(8)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize = 20)\n",
    "    CS = plt.contourf(xi, yi, zi)\n",
    "    g=plt.scatter(df.geometry.x, df.geometry.y, c=df.probability, s=60, vmin=0, vmax=1,cmap=cm.summer)\n",
    "    ax.add_artist(g)\n",
    "    #cbar = plt.colorbar() \n",
    "    #cbar.ax.tick_params(labelsize=20)\n",
    "        \n",
    "    patches=[]\n",
    "    # carico zone non ammesse per una certa percentuale\n",
    "    for index,value in unadmissible_area.items():\n",
    "        poly = mpatches.Polygon(value.exterior.coords)\n",
    "        patches.append(poly)\n",
    "    collection = PatchCollection(patches,facecolor = 'black', edgecolor = 'black', alpha=0.5)\n",
    "    ax.add_collection(collection)\n",
    "        \n",
    "    #plt.show()\n",
    "    fig.savefig(path, dpi  = 300)\n",
    "    \n",
    "    \n",
    "def coverage_heatmap_circle (df,C,pathToWrite,box,unadmissible_area):\n",
    "    boundsBox = box.bounds\n",
    "    minlat=boundsBox[1]\n",
    "    maxlat=boundsBox[3]\n",
    "    minlon=boundsBox[0]\n",
    "    maxlon=boundsBox[2]\n",
    "    #calcoli per heatmap\n",
    "    X = df.geometry.x.to_numpy()\n",
    "    Y = df.geometry.y.to_numpy()\n",
    "    Z = df.probability.to_numpy()\n",
    "    xi = np.linspace(minlon,maxlon,30)\n",
    "    yi = np.linspace(minlat,maxlat,30)\n",
    "    xi, yi = np.meshgrid(xi, yi)\n",
    "    x, y, z = df.geometry.x, df.geometry.y , df.probability\n",
    "    points = np.vstack((x,y)).T \n",
    "    # Z is a matrix of x-y values\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize = 20)\n",
    "    fig.set_figwidth(15)\n",
    "    fig.set_figheight(8)\n",
    "    zi = griddata(points, Z, (xi,yi), method='linear',fill_value=0)\n",
    "    # Create the contour plot\n",
    "    CS = plt.contourf(xi, yi, zi)\n",
    "    g=plt.scatter(df.geometry.x, df.geometry.y, c=df.probability, s=60, vmin=0, vmax=1,cmap=\"summer\")\n",
    "    #cbar = plt.colorbar() \n",
    "    #cbar.ax.tick_params(labelsize=20)\n",
    "\n",
    "    #patches in cui inserirÃ² il cerchio\n",
    "    patches = []\n",
    "    for c in C :\n",
    "        #strCenter=\"(\"+str(round(c[0],3)) +\" , \" + str (round(c[1],3)) +\")\"\n",
    "        #ax.annotate(strCenter, xy=c, xytext=(c[0],c[1]) , color = 'darkred',fontsize=18)\n",
    "        ax.plot([c[0]],[c[1]],'2r',color='red',markersize=20)\n",
    "        #costruisco un numpy array che rappresenta un cerchio con centro C e raggio Dr geodetici\n",
    "        circle_points = GD.Geodesic().circle( lon=c[0] , lat=c[1] , radius = Dr)\n",
    "        #trasformo il numpy array che rappresenta il cerchio in un poligono e lo aggiungo al patches\n",
    "        circle_poly = mpatches.Polygon(circle_points,color='w',fill=False, facecolor='white')\n",
    "        patches.append(circle_poly)\n",
    "    \n",
    "    #areeProibite\n",
    "    patches1=[]\n",
    "    for index,value in unadmissible_area.items():\n",
    "        poly = mpatches.Polygon(value.exterior.coords)\n",
    "        patches1.append(poly)\n",
    "    \n",
    "    collection = PatchCollection(patches,facecolor = 'grey', edgecolor = 'grey', alpha=0.9)\n",
    "    collection1 = PatchCollection(patches1,facecolor = 'black', edgecolor = 'black', alpha=0.3)\n",
    "\n",
    "\n",
    "\n",
    "    ax.add_artist(g)\n",
    "    ax.add_collection(collection1)\n",
    "    ax.add_collection(collection)\n",
    "\n",
    "    #plt.show()\n",
    "    fig.savefig(pathToWrite+\"k_\"+str(len(C))+\"_Dr=\"+str(Dr)+\".png\", dpi  = 300)\n",
    "    \n",
    "    \n",
    "\n",
    "#stampa dei risultati e scrittura su file\n",
    "def format_output(centers,n_covered_points,covered_points,nIterations,maxIterations,n_points,k,path, results,unadmissible_value,region):\n",
    "    file = open(path+\"k_\"+str(k)+\"_Dr_\"+str(Dr)+\".txt\",\"w\")\n",
    "    file.write(\"k=\"+str(k)+\"\\n\")\n",
    "    file.write(\"centers=\"+str(centers)+\"\\n\")\n",
    "    file.write(\"covered_points=\"+str(n_covered_points)+\"\\n\")\n",
    "    file.write(\"tot_points=\"+str(n_points)+\"\\n\")\n",
    "    file.write(\"covered_points_ratio=\"+str(n_covered_points/n_points)+\"\\n\")\n",
    "    file.write(\"iterations=\"+str(nIterations)+\"\\n\")\n",
    "    file.write(\"iterations_ratio=\"+str(nIterations/maxIterations)+\"\\n\")\n",
    "    file.write(\"locations:\\n\")\n",
    "    file.write(str(covered_points))\n",
    "    file.close()\n",
    "    # Write the overall result file\n",
    "    # algo name, period, ratio, n_points, n_covered_points,k stations,unadmissible_value,region(HD,LD),n_iterations, iteration ratio\n",
    "    results.write(\"StationPositioning,\"+suffix+\",\"+str(n_covered_points/n_points)+\",\"+str(n_points)+\",\"+str(n_covered_points)+\",\"+str(k)+\",\"+str(unadmissible_value)+\",\"+str(region)+\",\"+str(nIterations)+\",\"+str(nIterations/maxIterations)+\"\\n\")\n",
    "\n",
    "#controllo che il centro calcolato non ricada in un area proibita\n",
    "def is_invalid_center(C,unadmissible_area):\n",
    "    center = Point(C[0],C[1])\n",
    "    for index,u_area in unadmissible_area.items():\n",
    "        if u_area.intersects(center):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b = geopandas.read_file(pathDFLow+\"DFLow.geojson\",index_col = 0,columns=['probability','geometry'])\n",
    "#dfLow=geopandas.GeoDataFrame(b,crs=\"EPSG:4326\")\n",
    "\n",
    "#b = geopandas.read_file(pathDFNormal+\"DFNormal.geojson\",index_col = 0,columns=['probability','geometry'])\n",
    "#dfNormal = geopandas.GeoDataFrame(b,crs=\"EPSG:4326\")\n",
    "\n",
    "b = geopandas.read_file(pathDFHigh+\"DFHigh_\"+suffix+\".geojson\",index_col = 0,columns=['probability','geometry'])\n",
    "dfHigh=geopandas.GeoDataFrame(b,crs=\"EPSG:4326\")\n",
    "\n",
    "b = geopandas.read_file(pathDFBorder+\"DFBorder_\"+suffix+\".geojson\",index_col = 0,columns=['probability','geometry'])\n",
    "dfBorder=geopandas.GeoDataFrame(b,crs=\"EPSG:4326\")\n",
    "\n",
    "scenarios = [(dfHigh,boxHigh,pathDFHigh,\"HD\"),(dfBorder, boxBorder,pathDFBorder,\"LD\")]\n",
    "\n",
    "#print(\"DFLOW\", len(dfLow))\n",
    "#print(\"DFNORMAL\", len(dfNormal))\n",
    "print(\"DFHIGH\", len(dfHigh))\n",
    "print(\"DFBORDER\", len(dfBorder))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. StationPositioning\n",
    "- itero sugli scenario\n",
    "- itero sul numero di stazioni\n",
    "- itero sulla percentuale di zona non ammessa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = open(pathGeneral+\"results/results.txt\",\"a\")\n",
    "for scenario in scenarios:\n",
    "    for k in uav_stations:\n",
    "        for unadmissible_value in unadmissible_values:\n",
    "            print(\"Processing:\",scenario[3]+\"-\"+str(k))\n",
    "            dfS = scenario[0]\n",
    "            boxS = scenario[1]\n",
    "            pathS = scenario[2] \n",
    "            basePath = os.path.join(pathS,\"station_positioning/\",str(unadmissible_value)+\"/\")\n",
    "            print(basePath)\n",
    "            # stampo heatmap iniziali\n",
    "            #coverage_heatmap(dfS,geopandas.GeoSeries(),boxS,pathS+\"withoutAreeProibite\")\n",
    "            #coverage_heatmap(dfS,areeProibite,boxS,basePath+coverage_heatmap+\".png\")\n",
    "            \n",
    "            [centers,radius,n_covered_points,covered_points,nIterations,maxIterations,n_points,k] = KStationPositioning(dfS,boxS,k,unadmissible_areas[str(unadmissible_value)])\n",
    "            print(\"Kstation completed\")\n",
    "            #import pdb; pdb.set_trace()                    \n",
    "            dfNew = newDfCoverage(dfS,covered_points)            \n",
    "            print(\"NeDF completed\")\n",
    "            format_output(centers,n_covered_points,covered_points,nIterations,maxIterations,n_points,k,basePath,result_file,unadmissible_value,scenario[3])\n",
    "            coverage_heatmap_circle(dfS,centers,basePath,boxS,unadmissible_areas[str(unadmissible_value)])\n",
    "            \n",
    "result_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_positioning(df_points,box,k,basePath,dfOrig,unadmissible_area) :\n",
    "    boundsBox = box.bounds\n",
    "    minlat=boundsBox[1]\n",
    "    maxlat=boundsBox[3]\n",
    "    minlon=boundsBox[0]\n",
    "    maxlon=boundsBox[2]\n",
    "    mat = np.zeros((R0_iterations,2*k))\n",
    "\n",
    "    #genero lon e lat casuali delle stazioni e controllo che siano valide\n",
    "    for x in range(R0_iterations) : \n",
    "        i = 0\n",
    "        while i<k :\n",
    "            lon = np.random.uniform (minlon,maxlon,1)\n",
    "            lat = np.random.uniform (minlat,maxlat,1)\n",
    "            if is_invalid_center((lon,lat),unadmissible_area) == False:\n",
    "                mat[x][i*2] = lon\n",
    "                mat [x][i*2+1] = lat\n",
    "                i = i+1\n",
    "    #costruisco un dataframe con le posizioni generate casualmente\n",
    "    stations_locations = pd.DataFrame(mat)\n",
    "    results = pd.DataFrame(columns=[\"covered_points\",\"tot_points\", \"k\"])\n",
    "\n",
    "    #conteggio i punti coperti dalle varie stazioni\n",
    "    for row in stations_locations.iterrows():\n",
    "        circles = []\n",
    "        centers = []\n",
    "        for v in range(k):\n",
    "            #import pdb; pdb.set_trace()                    \n",
    "            circle_poly = Polygon (GD.Geodesic().circle( lon= row[1][2*v], lat=row[1][2*v+1], radius = Dr))\n",
    "            center = (row[1][2*v],row[1][2*v+1])\n",
    "            centers.append(center)\n",
    "            circles.append(circle_poly)\n",
    "        df_circles = geopandas.GeoDataFrame({'geometry':circles},crs=\"EPSG:4326\")        \n",
    "        #df_circles.plot()\n",
    "        df = geopandas.sjoin(df_points, df_circles, how = \"inner\", op=\"intersects\")\n",
    "        df.drop_duplicates(subset=['geometry'],inplace = True)\n",
    "        results = results.append({'covered_points': len(df), 'tot_points':len(df_points), 'k':k}, ignore_index=True)\n",
    "    #coverage_heatmap_circle(dfOrig,centers,basePath,box,unadmissible_area)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_random():\n",
    "    for scenario in scenarios:\n",
    "        dfS = scenario[0]\n",
    "        boxS = scenario[1]\n",
    "        pathS = scenario[2]\n",
    "        df_filtered = dfS[(dfS.probability<=Cr)]\n",
    "        for unadmissible_value in unadmissible_values:\n",
    "            basePath = os.path.join(pathS,\"random_positioning/\",str(unadmissible_value)+\"/\")\n",
    "            print(basePath)\n",
    "            res = []\n",
    "            for k in uav_stations:\n",
    "                print(\"Processing:\",scenario[3]+\"-\"+str(k)+\"-\"+str(unadmissible_value))\n",
    "                res.append(random_positioning(df_filtered, boxS, k,basePath, dfS,unadmissible_areas[str(unadmissible_value)]))        \n",
    "            pd.concat(res).to_csv(basePath+\"results.txt\",index=False)\n",
    "    \n",
    "    \n",
    "def run_random_results():\n",
    "    result_file = open(pathGeneral+\"results/results.txt\",\"a\")\n",
    "    # Analisi dei risultati\n",
    "    for scenario in scenarios:\n",
    "        pathS = scenario[2]\n",
    "        for unadmissible_value in unadmissible_values:\n",
    "            basePath = os.path.join(pathS,\"random_positioning/\",str(unadmissible_value)+\"/\")\n",
    "            r = pd.read_csv(basePath+\"results.txt\")\n",
    "            for k in uav_stations:\n",
    "                print(\"Results of: \",scenario[3]+\"-\"+str(k)+\"-\"+str(unadmissible_value))\n",
    "                r_f  = r[r[\"k\"]==k]\n",
    "                #import pdb; pdb.set_trace()                    \n",
    "                tot = r_f[\"tot_points\"].values[0]\n",
    "                #print(r_f.head())\n",
    "                print(\"mean convered\", r_f[\"covered_points\"].mean())\n",
    "                print(\"ratio convered\", r_f[\"covered_points\"].mean()/tot)\n",
    "                print(\"max convered\", r_f[\"covered_points\"].max())\n",
    "                print(\"std convered\", r_f[\"covered_points\"].std())\n",
    "                #write to the overall result file\n",
    "                #write to the overall result file\n",
    "                result_file.write(\"Random,\"+suffix+\",\"+str(r_f[\"covered_points\"].mean()/tot)+\",\"+str(tot)+\",\"+str(r_f[\"covered_points\"].mean())+\",\"+str(k)+\",\"+str(unadmissible_value)+\",\"+str(scenario[3])+\",\"+str(\"\")+\",\"+str(\"\")+\"\\n\")\n",
    "\n",
    "    result_file.close()\n",
    "run_random()\n",
    "run_random_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centeroidnp(arr):\n",
    "    length = arr.shape[0]\n",
    "    sum_x = np.sum(arr[:, 0])\n",
    "    sum_y = np.sum(arr[:, 1])\n",
    "    return sum_x/length, sum_y/length\n",
    "\n",
    "def dbscan_positioning(df_points,box,k,basePath,df_orig,unadmissible_area):\n",
    "    results = pd.DataFrame(columns=[\"covered_points\",\"tot_points\", \"k\"])\n",
    "    \n",
    "    X = df_points.geometry.x.to_numpy()\n",
    "    Y = df_points.geometry.y.to_numpy()\n",
    "    # 0.001Â° =111 m \n",
    "    # 0.005 = 555 \n",
    "    # https://www.usna.edu/Users/oceano/pguth/md_help/html/approx_equivalents.htm\n",
    "    clusterer = DBSCAN(metric=\"haversine\",min_samples=2,eps=0.003)\n",
    "    clusterer.fit(np.array([X,Y]).T)\n",
    "    \n",
    "    labels = [i for i in clusterer.labels_ if i >= 0]\n",
    "    n_lables = len(set(labels))\n",
    "    print(\"Found #clusters: \",n_lables)\n",
    "    #assegno la label del cluster ad ogni punto nel df originale\n",
    "    df_points[\"cluster\"] = clusterer.labels_\n",
    "    top_k_clusters = df_points.groupby(by=\"cluster\").count().reset_index().sort_values(by=\"probability\",ascending=False)\n",
    "    # remove noise samples\n",
    "    top_k_clusters = top_k_clusters[top_k_clusters[\"cluster\"] > -1]\n",
    "    top_k_clusters = top_k_clusters[[\"cluster\",\"probability\"]]\n",
    "\n",
    "    #build centroid\n",
    "    centers = []\n",
    "    selected = -1\n",
    "    # iterate over the first k clusters orderd in descending order\n",
    "    #import pdb; pdb.set_trace()                    \n",
    "    for i in top_k_clusters[\"cluster\"]:\n",
    "        if(selected == k-1):\n",
    "            # I selected the first k cluster and I now stop\n",
    "            break\n",
    "        cluster = df_points[df_points[\"cluster\"] == i]\n",
    "        size = len(cluster)\n",
    "        X = np.sum(cluster.geometry.x.to_numpy())\n",
    "        Y = np.sum(cluster.geometry.y.to_numpy())\n",
    "        lon_center = X/size\n",
    "        lat_center = Y/size\n",
    "        center = (lon_center, lat_center)\n",
    "        # check if this is a valid center\n",
    "        if(is_invalid_center(center,unadmissible_area)):\n",
    "            print(\"Invalid center: \",center)\n",
    "            continue\n",
    "        selected+=1\n",
    "        centers.append(center)\n",
    "        # radius in meters\n",
    "        circle_poly = Polygon (GD.Geodesic().circle( lon= lon_center, lat=lat_center, radius = Dr))\n",
    "        df_circle = geopandas.GeoDataFrame({'geometry':[circle_poly]},crs=\"EPSG:4326\")\n",
    "        # covered points\n",
    "        df = geopandas.sjoin(df_points, df_circle, how = \"inner\", op=\"intersects\")\n",
    "        df.drop_duplicates(subset=['geometry'],inplace = True)\n",
    "        #import pdb; pdb.set_trace()                    \n",
    "        results = results.append({'covered_points': len(df), 'tot_points':len(df_points), 'k':k}, ignore_index=True)\n",
    "    coverage_heatmap_circle(df_orig,centers,basePath,box,unadmissible_area)\n",
    "    return [results,top_k_clusters]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenarios = [(dfHigh,boxHigh,pathDFHigh,\"High\"),(dfBorder, boxBorder,pathDFBorder,\"Border\")]\n",
    "def run_dbscan():\n",
    "    for scenario in scenarios:\n",
    "        dfS = scenario[0]\n",
    "        boxS = scenario[1]\n",
    "        pathS = scenario[2]\n",
    "        df_filtered = dfS[(dfS.probability<=Cr)]\n",
    "        for unadmissible_value in unadmissible_values:\n",
    "            basePath = os.path.join(pathS,\"dbscan_positioning/\",str(unadmissible_value)+\"/\")\n",
    "            print(basePath)\n",
    "            res = []\n",
    "            for k in uav_stations:\n",
    "                print(\"Processing:\",scenario[3]+\"- K = \"+str(k)+\"- perc = \"+str(unadmissible_value))\n",
    "                [results, top_k_clusters] = dbscan_positioning(df_filtered, boxS, k,basePath,dfS,unadmissible_areas[str(unadmissible_value)])\n",
    "                res.append(results)\n",
    "            pd.concat(res).to_csv(basePath+\"results.txt\",index=False)\n",
    "            top_k_clusters.to_csv(basePath+\"dbscan_clusters_K_\"+str(k)+\"Dr=\"+str(Dr)+\".txt\",index=False)\n",
    "    \n",
    "def run_dbscan_analysis():\n",
    "    result_file = open(pathGeneral+\"results/results.txt\",\"a\")\n",
    "    # Analisi dei risultati\n",
    "    for scenario in scenarios:\n",
    "        pathS = scenario[2]\n",
    "        for unadmissible_value in unadmissible_values:\n",
    "            basePath = os.path.join(pathS,\"dbscan_positioning/\",str(unadmissible_value)+\"/\")\n",
    "            r = pd.read_csv(basePath+\"/results.txt\")\n",
    "            for k in uav_stations:\n",
    "                print(\"Results of: \",scenario[3]+\"-\"+str(unadmissible_value)+\"-\"+str(k)+\"-\")\n",
    "                r_f = r[r[\"k\"]==k]\n",
    "                tot = r_f[\"tot_points\"].values[0]\n",
    "                #print(r_f.head())\n",
    "                print(\"Mean coveredpoints: \",r_f[\"covered_points\"].mean())\n",
    "                print(\"Ratio of covered points: \",r_f[\"covered_points\"].mean()/tot)\n",
    "                #write to the overall result file\n",
    "                result_file.write(\"DBSCAN,\"+suffix+\",\"+str(r_f[\"covered_points\"].mean()/tot)+\",\"+str(tot)+\",\"+str(r_f[\"covered_points\"].mean())+\",\"+str(k)+\",\"+str(unadmissible_value)+\",\"+str(scenario[3])+\",\"+str(\"\")+\",\"+str(\"\")+\"\\n\")\n",
    "    result_file.close()\n",
    "run_dbscan()\n",
    "run_dbscan_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centeroidnp(arr):\n",
    "    length = arr.shape[0]\n",
    "    sum_x = np.sum(arr[:, 0])\n",
    "    sum_y = np.sum(arr[:, 1])\n",
    "    return sum_x/length, sum_y/length\n",
    "\n",
    "def kmeans_positioning(df_points,box,k,base_path,df_orig,unadmissible_area) :\n",
    "    results = pd.DataFrame(columns=[\"covered_points\",\"tot_points\", \"k\"])\n",
    "    \n",
    "    X = df_points.geometry.x.to_numpy()\n",
    "    Y = df_points.geometry.y.to_numpy()\n",
    "    # 0.006\n",
    "    clusterer = KMeans(n_clusters=k)\n",
    "    clusterer.fit(np.array([X,Y]).T)\n",
    "    \n",
    "    labels = [i for i in clusterer.labels_ if i >= 0]\n",
    "    n_lables = len(set(labels))\n",
    "\n",
    "    print(\"clusters: \",n_lables)\n",
    "    #assegno la label del cluster ad ogni punto nel df originale\n",
    "    df_points[\"cluster\"] = clusterer.labels_\n",
    "    top_k_clusters = df_points.groupby(by=\"cluster\").count().reset_index().sort_values(by=\"probability\",ascending=False)[[\"cluster\",\"probability\"]]\n",
    "    #top_k_clusters.to_csv(basePath+\"k_\"+str(k)+\"_kmeans_clusters.txt\",index=False)\n",
    "    #build centroid\n",
    "    centers = []\n",
    "    selected = -1\n",
    "    for i in top_k_clusters[\"cluster\"]:\n",
    "        if(selected == k-1):\n",
    "            # I selected the first k cluster and I now stop\n",
    "            break\n",
    "        cluster = df_points[df_points[\"cluster\"] == i]\n",
    "        size = len(cluster)\n",
    "        X = np.sum(cluster.geometry.x.to_numpy())\n",
    "        Y = np.sum(cluster.geometry.y.to_numpy())\n",
    "        lon_center = X/size\n",
    "        lat_center = Y/size\n",
    "        center = (lon_center, lat_center)\n",
    "        # check if this is a valid center\n",
    "        if(is_invalid_center(center,unadmissible_area)):\n",
    "            print(\"Invalid center: \",center)\n",
    "            continue\n",
    "        selected+=1\n",
    "        centers.append(center)\n",
    "\n",
    "        circle_poly = Polygon (GD.Geodesic().circle( lon= lon_center, lat=lat_center, radius = Dr))\n",
    "        df_circle = geopandas.GeoDataFrame({'geometry':[circle_poly]},crs=\"EPSG:4326\")        \n",
    "        df = geopandas.sjoin(df_points, df_circle, how = \"inner\", op=\"intersects\")\n",
    "        df.drop_duplicates(subset=['geometry'],inplace = True)\n",
    "        #import pdb; pdb.set_trace()                    \n",
    "        results = results.append({'covered_points': len(df), 'tot_points':len(df_points), 'k':k}, ignore_index=True)\n",
    "    coverage_heatmap_circle(df_orig,centers,basePath,box,unadmissible_area)\n",
    "    return [results,top_k_clusters]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_km():\n",
    "    for scenario in scenarios:\n",
    "        dfS = scenario[0]\n",
    "        boxS = scenario[1]\n",
    "        pathS = scenario[2]\n",
    "        df_filtered = dfS[(dfS.probability<=Cr)]\n",
    "        for unadmissible_value in unadmissible_values:\n",
    "            basePath = os.path.join(pathS,\"kmeans_positioning/\",str(unadmissible_value)+\"/\")\n",
    "            print(basePath)\n",
    "            res = []\n",
    "            for k in uav_stations:\n",
    "                print(\"Processing:\",scenario[3]+\"- K = \"+str(k)+\"- perc = \"+str(unadmissible_value))\n",
    "                [results, top_k_clusters] = kmeans_positioning(df_filtered, boxS, k,basePath,dfS,unadmissible_areas[str(unadmissible_value)])\n",
    "                res.append(results)\n",
    "            pd.concat(res).to_csv(basePath+\"results.txt\",index=False)\n",
    "            top_k_clusters.to_csv(basePath+\"kmeans_clusters_K_\"+str(k)+\"Dr=\"+str(Dr)+\".txt\",index=False)\n",
    "\n",
    "def run_km_analysis():  \n",
    "    result_file = open(pathGeneral+\"results/results.txt\",\"a\")\n",
    "    # Analisi dei risultati\n",
    "    for scenario in scenarios:\n",
    "        pathS = scenario[2]\n",
    "        for unadmissible_value in unadmissible_values:\n",
    "            basePath = os.path.join(pathS,\"kmeans_positioning/\",str(unadmissible_value)+\"/\")\n",
    "            r = pd.read_csv(basePath+\"/results.txt\")\n",
    "            for k in uav_stations:\n",
    "                print(\"Results of: \",scenario[3]+\"-\"+str(unadmissible_value)+\"-\"+str(k))\n",
    "                r_f = r[r[\"k\"]==k]\n",
    "                tot = r_f[\"tot_points\"].values[0]\n",
    "                #print(r_f.head())\n",
    "                print(\"Mean covered points\",r_f[\"covered_points\"].mean())\n",
    "                print(\"Ratio of covered points: \",r_f[\"covered_points\"].mean()/tot)\n",
    "                print(\"tot:\",tot)\n",
    "                #write to the overall result file\n",
    "                result_file.write(\"KMeans,\"+suffix+\",\"+str(r_f[\"covered_points\"].mean()/tot)+\",\"+str(tot)+\",\"+str(r_f[\"covered_points\"].mean())+\",\"+str(k)+\",\"+str(unadmissible_value)+\",\"+str(scenario[3])+\",\"+str(\"\")+\",\"+str(\"\")+\"\\n\")\n",
    "    result_file.close()\n",
    "    \n",
    "run_km()\n",
    "run_km_analysis()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(\"/home/michele/Documenti/Progetti/Software/Crowdsensing/uav_station/output/coverage/scenario/mix/10.0/coverages_2008_7_1-2009_12_31.csv\")\n",
    "c = 0\n",
    "s = len(a)\n",
    "print(s)\n",
    "p = np.arange(0,s,1)\n",
    "a[\"id_location\"] = p\n",
    "a.to_csv(\"/home/michele/Documenti/Progetti/Software/Crowdsensing/uav_station/output/coverage/scenario/mix/10.0/coverages_2008_7_1-2009_12_31_IDS.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
