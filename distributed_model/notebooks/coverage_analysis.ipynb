{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ea916-a85e-459b-8697-5c9c05bcad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install imageio\n",
    "#! pip install PIL\n",
    "#! pip install pylab\n",
    "#! pip install pathlib\n",
    "#! pip install scikit-learn\n",
    "#! pip install d3blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba06fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "import os\n",
    "import imageio\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "from pylab import *\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial.distance import cityblock\n",
    "import statistics\n",
    "\n",
    "with open(\"conf.yaml\") as f:\n",
    "    conf = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "cov_series_path = conf[\"cov_series_path\"]\n",
    "start_date= conf[\"start_date\"]\n",
    "end_date= conf[\"end_date\"]\n",
    "out_path = conf[\"out_path\"]\n",
    "path_pois = conf[\"path_pois\"]\n",
    "scales = conf[\"scale_values\"]\n",
    "d_radius = conf[\"detour_radius\"]\n",
    "attack_path = conf[\"attack_path\"]\n",
    "path_covmap = conf[\"path_covmap\"]\n",
    "#ed_threshold = conf[\"euclidean_cov_threshold\"]\n",
    "\n",
    "\n",
    "d_radius"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02f954",
   "metadata": {},
   "source": [
    "# Utilty function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e753226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_centralized(scale, start, end):\n",
    "    path = \"../../output/coverage/scenario/mix/\"+str(scale)+\".0/coverages_\"+start+\"_\"+end+\".csv\"\n",
    "    print(\"cen\",path)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def load_distributed(scale, detour_radius, start, end, attack):\n",
    "    if attack is True:\n",
    "        path = attack_path+start+\"_\"+end+\"/coverage_\"+str(scale)+\"_\"+str(detour_radius)+\".csv\"\n",
    "    else:\n",
    "        path = out_path+start+\"_\"+end+\"/coverage_\"+str(scale)+\"_\"+str(detour_radius)+\".csv\"\n",
    "    print(\"dis\",path)\n",
    "    #cov = pd.read_csv(path).drop(columns=[\"Unnamed: 0\"])\n",
    "    cov = pd.read_csv(path)\n",
    "    cov = cov.join(pois_filtered.set_index('id_location'), on=\"id_location\", how=\"left\")\n",
    "    #cov = cov.dropna()\n",
    "    distributed_coverage = {'id_location':[], 'probability':[]}\n",
    "    # compute the joint probability for each of the POIs\n",
    "    print(len(cov.id_location.unique()))\n",
    "    for poi in cov.id_location.unique():\n",
    "    \n",
    "        probs = cov[cov.id_location == poi]['probability']\n",
    "        product = 1\n",
    "        for p in probs:\n",
    "            product *= (1-float(p))\n",
    "        product = 1 - product    \n",
    "        distributed_coverage['id_location'].append(poi)\n",
    "        distributed_coverage['probability'].append(product)\n",
    "        distributed_coverage['scale'] = scale\n",
    "        distributed_coverage['detour_radius'] = detour_radius\n",
    "        \n",
    "    distributed_coverage = pd.DataFrame.from_dict(distributed_coverage).join(pois_filtered.set_index('id_location'), on=\"id_location\", how=\"left\")\n",
    "    \n",
    "    #MANFREDO save in a file dataframe with the joint probaility \n",
    "    if attack is True:\n",
    "        distributed_coverage.to_csv('joint_covmap_attack.csv', index=True)\n",
    "    else:\n",
    "        distributed_coverage.to_csv('joint_covmap_real.csv', index=True)\n",
    "    return distributed_coverage\n",
    "\n",
    "\n",
    "def load_swap(scale, detour_radius, start, end, attack):\n",
    "    if attack is False:\n",
    "        path = out_path+\"swap/no_filter/\"+start+\"_\"+end+\"/coverage_\"+str(scale)+\"_\"+str(detour_radius)+\".csv\"\n",
    "        cov=pd.read_csv(path) ##prova\n",
    "        print(cov['uid'].unique())\n",
    "    else: \n",
    "        path = attack_path+start+\"_\"+end+\"/coverage_\"+str(scale)+\"_\"+str(detour_radius)+\".csv\"\n",
    "        #meccanismo blaclist\n",
    "        path_bl = attack_path+start+\"_\"+end+\"/blacklists_\"+str(scale)+\"_\"+str(detour_radius)+\".pk1\"\n",
    "        cov = exclude_attackers(path_bl,path)\n",
    "        #cov=pd.read_csv(path) #to not use the blacklist mechanism\n",
    "        \n",
    "    print(\"dis\",path)\n",
    "    #cov = pd.read_csv(path).drop(columns=[\"Unnamed: 0\"])\n",
    "    #cov = pd.read_csv(path)  questa originale\n",
    "    cov = cov.join(pois_filtered.set_index('id_location'), on=\"id_location\", how=\"left\")\n",
    "    #cov = cov.dropna()\n",
    "    distributed_coverage = {'id_location':[], 'probability':[]}\n",
    "    # compute the joint probability for each of the POIs\n",
    "    \n",
    "    for poi in cov.id_location.unique():\n",
    "    \n",
    "        probs = cov[cov.id_location == poi]['probability']\n",
    "        product = 1\n",
    "        for p in probs:\n",
    "            product *= (1-float(p))\n",
    "        product = 1 - product    \n",
    "        distributed_coverage['id_location'].append(poi)\n",
    "        distributed_coverage['probability'].append(product)\n",
    "        distributed_coverage['scale'] = scale\n",
    "        distributed_coverage['detour_radius'] = detour_radius\n",
    "        \n",
    "    distributed_coverage = pd.DataFrame.from_dict(distributed_coverage).join(pois_filtered.set_index('id_location'), on=\"id_location\", how=\"left\")\n",
    "    #MANFREDO save in a file dataframe with the joint probaility \n",
    "    if attack is True:\n",
    "        distributed_coverage.to_csv('swap_covmap_attack.csv', index=True)\n",
    "    #else:\n",
    "    #    distributed_coverage.to_csv(path_covmap, index=True)\n",
    "    return distributed_coverage\n",
    "\n",
    "\n",
    "def plot_model(coverage, H, prefix):\n",
    "    numcols, numrows = 100, 100\n",
    "    dist = plt.figure(figsize=(10,10))\n",
    "    #ax = dist.gca(projection='3d')\n",
    "    ax=dist.add_subplot(projection='3d')\n",
    "\n",
    "    xi = np.linspace(coverage.lon.min(), coverage.lon.max(), numrows)\n",
    "    yi = np.linspace(coverage.lat.min(), coverage.lat.max(), numcols)\n",
    "\n",
    "    #print(xi, yi)\n",
    "\n",
    "    xi, yi = np.meshgrid(xi, yi)\n",
    "\n",
    "\n",
    "    x, y, z = coverage.lon, coverage.lat, coverage.probability#/H\n",
    "\n",
    "    # known points (long, lat)\n",
    "    points = np.vstack((x,y)).T \n",
    "    # known z points\n",
    "    values = z\n",
    "    # all the points I want to interpolate\n",
    "    wanted = (xi, yi)\n",
    "    # meshgrid interpolated\n",
    "    zi = griddata(points, values, wanted,method=\"linear\", fill_value = 0) \n",
    "\n",
    "    surf = ax.plot_surface(xi, yi, zi, cmap=cm.viridis,\n",
    "                           linewidth=0, antialiased=False,shade=True)\n",
    "    ax.set_xlabel('Longitude', fontsize=28, rotation=60, linespacing=10)\n",
    "    ax.set_ylabel('Latitude', fontsize=28, rotation=60, linespacing=10)\n",
    "    ax.set_zlabel('Coverage prob.', fontsize=28, rotation=60, linespacing=10)\n",
    "    plt.xticks(fontsize=25)\n",
    "    plt.yticks(fontsize=25)\n",
    "    \n",
    "    ax.xaxis.labelpad = 23\n",
    "    ax.yaxis.labelpad = 23\n",
    "    ax.zaxis.labelpad = 23\n",
    "\n",
    "    \n",
    "    \n",
    "    ax.set_zlim(0, coverage.probability.max())#/H)\n",
    "    #ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    #ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "    # Add a color bar which maps values to colors.\n",
    "    #fig.colorbar(surf, shrink=0.3, aspect=5)\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.savefig(\"images/\"+prefix+\"_3D.png\", dpi  = 100)\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10, 10)\n",
    "    plt.contourf(xi, yi, zi, cmap = cm.viridis)\n",
    "    # plt.scatter(centralized_coverage.lon, centralized_coverage.lat, c=centralized_coverage.probability, s=60, vmin=zi.min(), vmax=zi.max(),cmap=cm.Reds)\n",
    "    plt.ticklabel_format(useOffset=False)\n",
    "    plt.colorbar().set_label(label='Coverage prob.',size=25)\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel(\"Longitude\",fontsize=25)\n",
    "    plt.ylabel(\"Latitude\",fontsize=25)\n",
    "    plt.savefig(\"images/\"+prefix+\"_heatmap.png\", dpi  = 100, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "#utility\n",
    "def euclidean_distance(covmap):\n",
    "    df_final = pd.read_csv(path_covmap)\n",
    "    probability_final = np.array(df_final.sort_values(by=\"id_location\")['probability'])\n",
    "    #cov_to_test = np.array(covmap.sort_index(axis=0)[\"probability\"])\n",
    "    cov_to_test = np.array(covmap.sort_values(by='id_location')[\"probability\"])\n",
    "    #cambio scala, probability*100\n",
    "    #for y in range(0,1954):\n",
    "    #    probability_final[y]=probability_final[y]*100\n",
    "    #    cov_to_test[y] = cov_to_test[y]*100\n",
    "\n",
    "    return np.linalg.norm(probability_final - cov_to_test)\n",
    "\n",
    "#function to find attackers from blacklists and exclude 20% of them for the coverage map final\n",
    "#input:  path blacklists and path coverage, output df of coverage_scale_dt_radius \"cleaned\"\n",
    "def exclude_attackers(path_blacklist, path_covmap_final):\n",
    "    cov = pd.read_csv(path_covmap_final)\n",
    "    merged = []\n",
    "    all_users=[]\n",
    "    with open(path_blacklist, \"rb\") as fp: \n",
    "        bl = pickle.load(fp)\n",
    "        #print(\"blacklist recovered\")\n",
    "        print(bl)\n",
    "        all_users = list(bl.keys())\n",
    "        bl_list = list(bl.values())\n",
    "        merged = list(itertools.chain(*bl_list))\n",
    "\n",
    "    list_user = np.unique(merged)\n",
    "    list_user = list(list_user)\n",
    "    print(\"USER IN BL: \",list_user)\n",
    "    #retrieve means\n",
    "    dict_mh = {}\n",
    "    dict_ed = {}\n",
    "    final_covmap = pd.read_csv(path_covmap)\n",
    "    print(\"ALL USERS: \", all_users)\n",
    "    #max_att = int(np.round(len(all_users)*20/100)) #recover the max of attackers (20%)\n",
    "    max_att = 4\n",
    "    print(\"MAX ATTACKERS: \", str(max_att))\n",
    "    all_users = cov['uid'].unique()\n",
    "\n",
    "    #eliminate node in blacklist if he sends to the server a blank coverage map\n",
    "    for node in list_user:\n",
    "        if node not in all_users:\n",
    "            print(\"NODO: \",node)\n",
    "            list_user.remove(node)\n",
    "    for user in all_users:\n",
    "        df_cov_user = cov[cov['uid']==user]\n",
    "        probability_final = np.array(final_covmap.sort_values(by=\"id_location\")['probability'])\n",
    "        cov_to_test = np.array(df_cov_user.sort_values(by='id_location')[\"probability\"])\n",
    "        dict_ed[int(user)] = np.linalg.norm(probability_final - cov_to_test)\n",
    "        dict_mh[int(user)] = cityblock(probability_final, cov_to_test)\n",
    "\n",
    "    #media delle manhattan e euclidean distance tra tutte le mappe che il server riceve\n",
    "    ed_mean = mean(list(dict_ed.values()))\n",
    "    mh_mean = mean(list(dict_mh.values()))\n",
    "    print(\"ed_mean: \",ed_mean)\n",
    "    print(\"mh_mean: \",mh_mean)\n",
    "\n",
    "    #sorted by value (distance)\n",
    "    sorted_dict_mh = sorted(dict_mh.items(), key=lambda x:x[1], reverse=True)\n",
    "    sorted_dict_ed = sorted(dict_ed.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    #check \n",
    "    counter = 0\n",
    "    for user in sorted_dict_mh:\n",
    "        if user[0] in list_user:\n",
    "            df_cov_user = cov[cov['uid']==user[0]]\n",
    "            print(\"USER : {} EUCLIDEAN DISTANCE: {}\",user[0],dict_ed[user[0]])\n",
    "            print(\"USER : {} MANHATTAN DISTANCE: {}\",user[0],dict_mh[user[0]])\n",
    "            \n",
    "            if ((dict_ed[user[0]] > ed_mean)            #mh_std/2 e ed_std/2\n",
    "                and (dict_mh[user[0]] > mh_mean)) :\n",
    "                print(\"elimino cov map di: \", user[0])\n",
    "                #remove coverage map of user from cov\n",
    "                for index, row in cov.iterrows():\n",
    "                     if row.uid == user[0]:\n",
    "                         cov = cov.drop(index)\n",
    "                counter += 1\n",
    "                if counter == max_att:\n",
    "                    break\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a98f9e-35bc-4c28-847f-4caf4732f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cella per salvare la giusta coverage map finale senza attacco\n",
    "#start = datetime.datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "start = \"2009-05-16\"\n",
    "end_date = [\"2009-05-23 00:00:00\"]\n",
    "pois_filtered = pd.read_csv(path_pois) \n",
    "for end in end_date:\n",
    "    end =  datetime.datetime.strptime(end, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "    swap_real = load_swap(100,800,start,end,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef4632e-9b75-41aa-835d-dd856f4ca5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mettere a confronto distanze delle coverage degli utenti nei 3 periodi\n",
    "scale = 100\n",
    "detour_radius = 800\n",
    "attackers = [30,128,84,140]\n",
    "pois_filtered = pd.read_csv(path_pois)  \n",
    "df_1 = pd.read_csv(attack_path+\"2009-05-08_2009-05-15/coverage_100_800.csv\") #recover period 1\n",
    "df_2 = pd.read_csv(attack_path+\"2009-05-16_2009-05-23/coverage_100_800.csv\") #recover period 2\n",
    "df_3 = pd.read_csv(attack_path+\"2009-05-24_2009-05-31/coverage_100_800.csv\") #recover period 3\n",
    "df_4 = pd.read_csv(attack_path+\"2009-05-08_2009-05-31/coverage_100_800.csv\") #recover period 4\n",
    "df_list = [df_1,df_2,df_3,df_4]\n",
    "\n",
    "counter = 1\n",
    "for df in df_list:\n",
    "    print(\"Period: \", counter)\n",
    "    list_user = df['uid'].unique()\n",
    "    print(list_user)\n",
    "    dict_ed = {}\n",
    "    dict_mh = {}\n",
    "    dicts = []\n",
    "    manhattan_threshold = 0\n",
    "    for user in list_user:\n",
    "        df_cov_user = df[df['uid']==user]\n",
    "        if counter == 1 or counter == 4:\n",
    "            final_covmap = pd.read_csv(\"swap_covmap_real.csv\")\n",
    "        elif counter == 2:\n",
    "            final_covmap = pd.read_csv(\"swap_covmap_real2.csv\")\n",
    "        else:\n",
    "            final_covmap = pd.read_csv(\"swap_covmap_real3.csv\")\n",
    "        probability_final = np.array(final_covmap.sort_values(by=\"id_location\")['probability'])\n",
    "        cov_to_test = np.array(df_cov_user.sort_values(by='id_location')[\"probability\"])\n",
    "        dict_ed[int(user)] = np.linalg.norm(probability_final - cov_to_test)\n",
    "        dict_mh[int(user)] = cityblock(probability_final, cov_to_test)\n",
    "    print(\"Mean Euclidean Distance:\", mean(list(dict_ed.values())))\n",
    "    print(\"Mean Manhattan Distance:\", mean(list(dict_mh.values())))\n",
    "    #print(\"Mean difference manhattan from manhattan mean:\", mean(np.absolute(np.array(list(dict_mh.values()))-mean(list(dict_mh.values())))))\n",
    "    print(\"STANDARD DEVIATION MANHATTAN: \",np.std(list(dict_mh.values())))\n",
    "    #print(\"Mean difference euclidean from euclidean mean:\", mean(np.absolute(np.array(list(dict_ed.values()))-mean(list(dict_ed.values())))))\n",
    "    print(\"STANDARD DEVIATION EUCLIDEAN: \",np.std(list(dict_ed.values())))\n",
    "    dicts.append(dict_ed)\n",
    "    dicts.append(dict_mh)\n",
    "    inner_counter = 1\n",
    "    for dictx in dicts:\n",
    "        #ordino dictionary per id utente\n",
    "        keys = list(dictx.keys())\n",
    "        keys.sort()\n",
    "        colors = []\n",
    "        i = 0\n",
    "        for key in keys:\n",
    "            if key in attackers:\n",
    "                colors.append(\"red\")\n",
    "            else:\n",
    "                colors.append(\"blue\")\n",
    "            i +=1\n",
    "            \n",
    "        sorted_dict = {i: dictx[i] for i in keys}\n",
    "        bins = range(len(sorted_dict.keys()))\n",
    "        plt.scatter(bins, sorted_dict.values(), c=colors)\n",
    "        plt.xlabel('Users', fontsize=16)\n",
    "        \n",
    "        plt.xticks(bins, sorted_dict.keys())\n",
    "        #for manhattan\n",
    "        if inner_counter == 2:\n",
    "            horizontal_line_sup = mean(list(dict_mh.values())) #+ np.std(list(dict_mh.values()))/2#mean(np.absolute(np.array(list(dict_mh.values()))-mean(list(dict_mh.values()))))  \n",
    "            #horizontal_line_inf = mean(list(dict_mh.values())) - mean(np.absolute(np.array(list(dict_mh.values()))-mean(list(dict_mh.values()))))\n",
    "            plt.ylabel('Manhattan Distance', fontsize=16)\n",
    "        #for euclidean\n",
    "        if inner_counter == 1:\n",
    "            horizontal_line_sup = mean(list(dict_ed.values())) #+ np.std(list(dict_ed.values()))/2 #mean(np.absolute(np.array(list(dict_ed.values()))-mean(list(dict_ed.values()))))  \n",
    "            #horizontal_line_inf = mean(list(dict_ed.values())) - mean(np.absolute(np.array(list(dict_ed.values()))-mean(list(dict_ed.values()))))\n",
    "            plt.ylabel('Euclidean Distance', fontsize=16)\n",
    "        plt.axhline(y=horizontal_line_sup, color='r', linestyle='--', label='Horizontal Line Sup')\n",
    "        #plt.axhline(y=horizontal_line_inf, color='r', linestyle='--', label='Horizontal Line Inf')\n",
    "        plt.show()\n",
    "        inner_counter += 1\n",
    "    #update counter\n",
    "    counter += 1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63104cb8-980a-46c4-a592-22c1976b48e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generazione matrice dei contatti\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from d3blocks import D3Blocks  #for chord graph\n",
    "\n",
    "start = \"2009-05-08\"\n",
    "end = \"2009-05-31\"\n",
    "\n",
    "# Example data: Replace this with your actual data\n",
    "contacts = pd.read_csv(attack_path+start+\"_\"+end+\"/contacts_100_800.csv\")\n",
    "\n",
    "#####################chord graph###############\n",
    "source = contacts['node_a']\n",
    "target = contacts['node_b']\n",
    "weights = contacts['contacts']\n",
    "df = pd.DataFrame(data=np.c_[source, target, weights], columns=['source','target','weight'])\n",
    "\n",
    "# Initialize\n",
    "d3 = D3Blocks(frame=False)\n",
    "d3.chord(df, color='source', opacity='source', cmap='Set2')\n",
    "\n",
    "#################################################\n",
    "all_users_a = contacts['node_a'].unique()\n",
    "all_users_b = contacts['node_b'].unique()\n",
    "all_users = np.union1d(all_users_a, all_users_b)\n",
    "#all_users = all_users.unique()\n",
    "print(\"ALL USERS: \", all_users)\n",
    "print(\"NUM USERS: \", len(all_users))\n",
    "num_users = len(all_users)\n",
    "contact_data = []\n",
    "for user_a in all_users:\n",
    "    user_a_contacts = []\n",
    "    for user_b in all_users:\n",
    "        if user_b != user_a:\n",
    "            if len(contacts.loc[contacts.node_a==user_a][contacts.node_b==user_b].values)!=0:\n",
    "                #print(contacts.loc[contacts.node_a==user_a][contacts.node_b==user_b].values[0][3])\n",
    "                user_a_contacts.append(contacts.loc[contacts.node_a==user_a][contacts.node_b==user_b].values[0][3])\n",
    "            else:\n",
    "                #print(contacts.loc[contacts.node_a==user_b][contacts.node_b==user_a].values[0][3])\n",
    "                user_a_contacts.append(contacts.loc[contacts.node_a==user_b][contacts.node_b==user_a].values[0][3])\n",
    "        else:\n",
    "            user_a_contacts.append(0)\n",
    "    contact_data.append(user_a_contacts)\n",
    "            \n",
    "print(contact_data)\n",
    "# Create the contact matrix\n",
    "contact_matrix = np.array(contact_data)\n",
    "print(contact_matrix)\n",
    "# Plotting the matrix with cell annotations\n",
    "plt.figure(figsize=(18, 16))\n",
    "plt.matshow(contact_matrix, fignum=1, cmap='viridis')  #DECIDERE BENE COLORE\n",
    "\n",
    "plt.xticks(np.arange(num_users))\n",
    "plt.yticks(np.arange(num_users))\n",
    "plt.gca().set_xticklabels(all_users)  # Adjust if users are numbered differently\n",
    "plt.gca().set_yticklabels(all_users)  # Adjust if users are numbered differently\n",
    "\n",
    "#fill the matrix\n",
    "for i in range(contact_matrix.shape[0]):\n",
    "    for j in range(contact_matrix.shape[1]):\n",
    "        plt.text(j, i, str(contact_matrix[i, j]), va='center', ha='center', color='red')\n",
    "\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(label='Number of contacts', size=15)\n",
    "plt.title('Contact Matrix', fontsize=20)\n",
    "plt.xlabel('Users', fontsize=18)\n",
    "plt.ylabel('Users', fontsize=18, rotation=90)\n",
    "#plt.savefig(\"08-31_contact_matrix_augmented.png\", dpi  = 100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce05e858-b34f-4ea2-94bc-ce78ed01691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#differenze principali POI \n",
    "start = \"2009-05-08\"\n",
    "end = \"2009-05-31\"\n",
    "scale = 100\n",
    "detour_radius = 800\n",
    "pois_filtered = pd.read_csv(path_pois)  \n",
    "H = len(pois_filtered)\n",
    "\n",
    "df_real = load_swap(scale,detour_radius,start,end,False)\n",
    "df_attack = load_swap(scale,detour_radius,start,end,True)\n",
    "\n",
    "df_real = df_real.sort_values(by=\"id_location\")\n",
    "df_attack = df_attack.sort_values(by=\"id_location\")\n",
    "#probability_final = np.array(df_real.sort_values(by=\"id_location\")['probability'])\n",
    "#probability_attack = np.array(df_attack.sort_values(by='id_location')['probability'])\n",
    "\n",
    "counter = 0\n",
    "POI_real = {}\n",
    "POI_attack = {}\n",
    "for index,row in df_real.iterrows():\n",
    "    #print(df_attack.loc[df_attack.id_location==row.id_location,'probability'].values[0])\n",
    "    prob_attack = df_attack.loc[df_attack.id_location==row.id_location,'probability'].values[0]\n",
    "    if np.absolute(row.probability-prob_attack)>=0.5:\n",
    "        POI_real[row.id_location] = row.probability\n",
    "        POI_attack[row.id_location] = prob_attack\n",
    "        counter+=1\n",
    "\n",
    "print(\"numero POI variati molto: \",counter)\n",
    "keys = POI_real.keys()  # Assuming both dictionaries have the same keys\n",
    "print(\"keys: \",keys)\n",
    "print(\"len keys: \", len(keys))\n",
    "values_real = POI_real.values()\n",
    "values_attack = POI_attack.values()\n",
    "\n",
    "x2 = np.arange(len(keys))\n",
    "\n",
    "plt.scatter(x2, values_real, label='No Attack')\n",
    "plt.scatter(x2, values_attack, label='Attack')\n",
    "\n",
    "plt.xlabel('ID location', fontsize=18)\n",
    "plt.ylabel('Values', fontsize=18)\n",
    "#plt.xticks(np.arange(len(keys)),keys, rotation=90)\n",
    "#plt.xticks(np.arange(0,len(keys),step=1), keys, rotation=90)\n",
    "#plt.tick_params(axis='x', which='major', labelsize=3)\n",
    "plt.xticks(x2, keys, rotation=90)\n",
    "\n",
    "plt.title('POI most changed')\n",
    "plt.legend()\n",
    "plt.figure(figsize=(18, 16))\n",
    "plt.show()\n",
    "#print(\"POI_REAL: \", POI_real)\n",
    "#print(\"POI_ATTACK: \", POI_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e2e73-c627-4acc-8daf-a2e10dcfc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate coverage map final by server with blacklist mechanism\n",
    "\n",
    "#start = datetime.datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "start = \"2009-05-08\"\n",
    "end_date = [\"2009-05-31 00:00:00\"]\n",
    "scale = 100\n",
    "detour_radius = 800\n",
    "pois_filtered = pd.read_csv(path_pois)  \n",
    "H = len(pois_filtered)\n",
    "\n",
    "#attackers= [128,30]\n",
    "#attackers=[30]\n",
    "for end in end_date:\n",
    "    end =  datetime.datetime.strptime(end, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "    #distributed_coverage = load_distributed(scale,detour_radius,start,end,False)\n",
    "    df_real = load_swap(scale,detour_radius,start,end,False) #sono swap senza attacco\n",
    "    swap_attack = load_swap(scale,detour_radius,start,end,True)\n",
    "    #df_distributed = pd.read_csv(\"joint_covmap_real.csv\")\n",
    "    df_swap = pd.read_csv(\"swap_covmap_attack.csv\")\n",
    "    #df_real = pd.read_csv(\"swap_covmap_real.csv\")\n",
    "\n",
    "    \n",
    "    real = df_real['probability']\n",
    "    attack = df_swap['probability']\n",
    "    bins = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "    #x = plt.hist(real, bins, label=['real'])\n",
    "    #y = plt.hist(attack, bins, label=['attack'])\n",
    "    plt.hist([real,attack], bins, label=['real','attack'])\n",
    "    plt.legend(loc='upper center')\n",
    "    plt.xlabel('Probability',fontsize=14)\n",
    "    plt.ylabel('Number of POI',fontsize=14)\n",
    "    plt.title(\"Period \"+start+\"/\"+end, fontsize=14)\n",
    "    plt.xticks(bins)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    probability_final = np.array(df_real.sort_values(by=\"id_location\")['probability'])\n",
    "    probability_attack = np.array(df_swap.sort_values(by='id_location')['probability'])\n",
    "    print(\"FINAL EUCLIDEAN DISTANCE: \",np.linalg.norm(probability_final - probability_attack) )\n",
    "    print(\"FINAL MANHATTAN DISTANCE: \", cityblock(probability_final,probability_attack))\n",
    "    df_real = df_real.dropna()\n",
    "    df_swap = df_swap.dropna()\n",
    "    plot_model(df_real,H, \"real\"+str(scale)+\"_\"+str(detour_radius)+\"_\"+str(start)+\"_\"+str(end))\n",
    "    plot_model(df_swap, H,  \"attack\"+str(scale)+\"_\"+str(detour_radius)+\"_\"+str(start)+\"_\"+str(end))\n",
    "\n",
    "    #plot the heatmap of the difference \n",
    "    print(\"PLOT DIFFERENCE:\")\n",
    "    df_difference = df_real.sort_values(by=\"id_location\")\n",
    "    df_difference['probability'] = df_swap.sort_values(by='id_location')['probability'] - df_difference['probability'] \n",
    "    plot_model(df_difference, H, \"difference\"+str(scale)+\"_\"+str(detour_radius)+\"_\"+str(start)+\"_\"+str(end))\n",
    "    \n",
    "    print(stats.ks_2samp(df_real.sort_values(by=\"id_location\")['probability'], df_swap.sort_values(by=\"id_location\")['probability']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def46c9e-8d4a-429a-bc61-a1c2ef0aa972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grafico per paragonare risultati con o senza filter e blacklist\n",
    "\n",
    "#euclidean\n",
    "list_blacklist = [5.18, 5.31, 4.52,4.73]\n",
    "list_no_blacklist = [16.15, 15.38, 4.55,11.23]\n",
    "list_no_defense = [20.12, 24.98, 17.11,20.25]\n",
    "\n",
    "#manhattan\n",
    "#list_blacklist = [86.1, 84.77, 54.32,73.54]\n",
    "#list_no_blacklist = [342.76, 312.48, 53.07,207.79]\n",
    "#list_no_defense = [529.76, 752.67, 393,581.93]\n",
    "\n",
    "x_axis = [\"Scenario 1\", \"Scenario 2\", \"Scenario 3\", \"Scenario 4\"]\n",
    "x_positions = np.arange(len(x_axis))\n",
    "\n",
    "plt.bar(x_positions - 0.3, list_no_defense, width=0.3, label='No Defense')\n",
    "plt.bar(x_positions - 0.00, list_no_blacklist, width=0.3, label='Filter')\n",
    "plt.bar(x_positions + 0.3, list_blacklist, width=0.3, label='Filter & Blacklist')\n",
    "\n",
    "plt.xlabel('Time Period', fontsize=16)\n",
    "plt.ylabel('Euclidean Distance', fontsize=16)\n",
    "plt.title('Filter & Blacklist results')\n",
    "plt.legend()\n",
    "\n",
    "# Setting x-axis ticks and labels\n",
    "plt.xticks(x_positions, x_axis, rotation=45)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b90d06d-677b-461c-bb97-ac7df14f6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph with final results for all scenarios with both attacks and both Malicious worker ratio\n",
    "#lists no defense\n",
    "random_ed_10_nodefense = [36.72,37.30,36.26,30.12]\n",
    "random_ed_20_nodefense = [36.73,37.33,36.48,30.12]\n",
    "random_mh_10_nodefense = [1507.69,1527.19,1470.70,1094.53]\n",
    "random_mh_20_nodefense = [1508.03,1528.56,1480.21,1094.53]\n",
    "PASL_ed_10_nodefense = [20.12,24.98,17.11,20.26]\n",
    "PASL_ed_20_nodefense = [19.87,19.45,14.23,20.95]\n",
    "PASL_mh_10_nodefense = [529.76,752.67,393,581.93]\n",
    "PASL_mh_20_nodefense = [526.68,498.75,312.59,615.55]\n",
    "\n",
    "#lists defense\n",
    "random_ed_10 = [3.33,3.54,3.94,4.47]\n",
    "random_ed_20 = [5.72,5.7,5.85,5.44]\n",
    "random_mh_10 = [45.38,42.36,46.17,64.32]\n",
    "random_mh_20 = [91.1,92.27,97.37,103.1]\n",
    "PASL_ed_10 = [5.18,5.31,4.52,4.73]\n",
    "PASL_ed_20 = [11.1,10.1,8.7,13.3]\n",
    "PASL_mh_10 = [86.1,84.77,54.32,73.54]\n",
    "PASL_mh_20 = [243,214.8,170,362.1]\n",
    "\n",
    "x_axis = [\"Scenario 1\", \"Scenario 2\", \"Scenario 3\", \"Scenario 4\"]\n",
    "x_positions = np.arange(len(x_axis))\n",
    "\n",
    "plt.plot(x_axis, random_ed_10_nodefense, label='Random_10_noDef', marker='o', ls=\"--\")\n",
    "plt.plot(x_axis, random_ed_20_nodefense, label='Random_20_noDef', marker='o', ls=\"--\")\n",
    "plt.plot(x_axis, PASL_ed_10_nodefense, label='PASL_10_noDef', marker='o', ls=\"--\")\n",
    "plt.plot(x_axis, PASL_ed_20_nodefense, label='PASL_20_noDef', marker='o', ls=\"--\")\n",
    "plt.plot(x_axis, random_ed_10, label='Random_10', marker='o')\n",
    "plt.plot(x_axis, random_ed_20, label='Random_20', marker='o')\n",
    "plt.plot(x_axis, PASL_ed_10, label='PASL_10', marker='o')\n",
    "plt.plot(x_axis, PASL_ed_20, label='PASL_20', marker='o')\n",
    "\n",
    "plt.xlabel('Scenario', fontsize=18)\n",
    "plt.ylabel('Euclidean Dist.', fontsize=18)\n",
    "plt.title(\"Final Result Euclidean Distance\", fontsize=18)\n",
    "plt.legend(loc=(1,0.5))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_axis, random_mh_10_nodefense, label='Random_10_noDef', marker='o', ls=\"--\")\n",
    "plt.plot(x_axis, random_mh_20_nodefense, label='Random_20_noDef', marker='o', ls=\"--\")\n",
    "plt.plot(x_axis, PASL_mh_10_nodefense, label='PASL_10_noDef', marker='o', ls=\"--\")\n",
    "plt.plot(x_axis, PASL_mh_20_nodefense, label='PASL_20_noDef', marker='o', ls=\"--\")\n",
    "plt.plot(x_axis, random_mh_10, label='Random_10', marker='o')\n",
    "plt.plot(x_axis, random_mh_20, label='Random_20', marker='o')\n",
    "plt.plot(x_axis, PASL_mh_10, label='PASL_10', marker='o')\n",
    "plt.plot(x_axis, PASL_mh_20, label='PASL_20', marker='o')\n",
    "\n",
    "plt.xlabel('Scenario', fontsize=18)\n",
    "plt.ylabel('Manhattan Dist.', fontsize=18)\n",
    "plt.title(\"Final Result Manhattan Distance\", fontsize=18)\n",
    "plt.legend(loc=(1,0.50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07e2579",
   "metadata": {},
   "source": [
    "## KS test centralized vs distributed\n",
    "- scale = 50\n",
    "- detour_radius = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe473c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 100 #50\n",
    "detour_radius = 800\n",
    "pois_filtered = pd.read_csv(path_pois)  \n",
    "H = len(pois_filtered)\n",
    "start = datetime.datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "\n",
    "end_date = [\"2009-05-15 00:00:00\"]\n",
    "for end in end_date:\n",
    "    end =  datetime.datetime.strptime(end, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "    centralized_coverage = load_centralized(scale, start, end)\n",
    "    distributed_coverage = load_distributed(scale,detour_radius,start,end, False)\n",
    "    print(stats.ks_2samp(distributed_coverage.sort_values(by=\"id_location\")['probability'], centralized_coverage.join(distributed_coverage.set_index('id_location'), on=\"id_location\", how=\"right\", rsuffix=\"_distr\").sort_values(by=\"id_location\")['probability']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9052f7ad",
   "metadata": {},
   "source": [
    "## KS test distributed vs Swap\n",
    "- 1 coverage map obtained as joint probability\n",
    "- KS distribution for every user\n",
    "- contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS using 1 coverage map given by the joint probability\n",
    "scale = 800\n",
    "detour_radius = 100\n",
    "pois_filtered = pd.read_csv(path_pois)  \n",
    "H = len(pois_filtered)\n",
    "start = datetime.datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "print(start)\n",
    "end_date = [\"2009-05-25 00:00:00\",\"2009-05-31 00:00:00\",\"2009-06-05 00:00:00\"]\n",
    "#end_date = [\"2009-05-07 00:00:00\", \"2009-05-15 00:00:00\"]\n",
    "for end in end_date:\n",
    "    end =  datetime.datetime.strptime(end, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "    swap_coverage = load_swap(scale,detour_radius,start,end,False)\n",
    "    distributed_coverage = load_distributed(scale,detour_radius,start,end,False)\n",
    "    print(stats.ks_2samp(distributed_coverage.sort_values(by=\"id_location\")['probability'], swap_coverage.join(distributed_coverage.set_index('id_location'), on=\"id_location\", how=\"right\", rsuffix=\"_distr\").sort_values(by=\"id_location\")['probability']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16842797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS(swap, distributed) distribution for every user\n",
    "#end_dates = [\"2009-05-12 00:00:00\",\"2009-05-14 00:00:00\",\"2009-05-16 00:00:00\",\"2009-05-17 00:00:00\",\"2009-05-20 00:00:00\",\"2009-05-25 00:00:00\",\"2009-05-31 00:00:00\",\"2009-06-05 00:00:00\"]\n",
    "end_dates = [\"2009-05-25 00:00:00\",\"2009-05-31 00:00:00\",\"2009-06-05 00:00:00\"]\n",
    "sns.set(style=\"ticks\",font_scale=2.6)\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "scale = 800\n",
    "detour_radius = 100\n",
    "ks_df = pd.DataFrame(columns=[\"statistic\",\"p-value\",\"uid\",\"scenario\"])\n",
    "start = datetime.datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "for end in end_dates:\n",
    "    end =  datetime.datetime.strptime(end, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "    distributed_path = out_path+start+\"_\"+end+\"/coverage_\"+str(scale)+\"_\"+str(detour_radius)+\".csv\"\n",
    "    swap_path = out_path+\"swap/\"+start+\"_\"+end+\"/coverage_\"+str(scale)+\"_\"+str(detour_radius)+\".csv\"\n",
    "\n",
    "    distributed_df = pd.read_csv(distributed_path)\n",
    "    swap_df = pd.read_csv(swap_path)\n",
    "    for user in distributed_df.uid.unique():\n",
    "        #print(user)\n",
    "        user_cov_dis = distributed_df[distributed_df.uid == user]\n",
    "        user_swap_dis = swap_df[swap_df.uid == user]\n",
    "        if user_swap_dis.empty:\n",
    "            continue\n",
    "        user_ks = stats.ks_2samp(user_cov_dis.sort_values(by=\"id_location\")['probability'], user_swap_dis.join(user_cov_dis.set_index('id_location'), on=\"id_location\", how=\"right\", rsuffix=\"_distr\").sort_values(by=\"id_location\")['probability'])\n",
    "        ks_df = ks_df.append({\"statistic\":user_ks[0],\"p-value\": user_ks[1] ,\"uid\":user,\"scenario\": end},ignore_index=True)\n",
    "\n",
    "\n",
    "#print(ks_df)\n",
    "\n",
    "# sns.jointplot(data=ks_df, x = \"KS\", y=\"p-value\", hue=\"scenario\",height=10)\n",
    "# plt.ylim([0,1.2])\n",
    "\n",
    "\n",
    "# pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\n",
    "# g = sns.FacetGrid(ks_df, row=\"scenario\", hue=\"scenario\", palette = pal,despine = True, aspect = 4,sharey=False,row_order=[\"2009-05-12\",\"2009-05-14\",\"2009-05-31\"])\n",
    "# g.map(sns.kdeplot, \"p-value\",bw_adjust=.5, clip_on=True,fill=True, alpha=1, linewidth=1.5)\n",
    "# g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n",
    "# g.set_xlabels('p-value')\n",
    "# g.set_yticklabels([])\n",
    "# g.despine(bottom=True, left=True)\n",
    "#######\n",
    "\n",
    "ks_df = pd.melt(ks_df,id_vars=['scenario'],var_name='metric', value_vars=[\"statistic\",\"p-value\"],value_name='values')\n",
    "#g = sns.FacetGrid(ks_df,hue=\"scenario\", col=\"metric\" , col_wrap=2, sharex=False,sharey=False,despine = True, palette=\"tab10\",col_order=[\"p-value\",\"statistic\"],hue_order=[\"2009-05-25\",\"2009-05-31\",\"2009-06-05\"],aspect=3,height=5)\n",
    "#g.map(sns.kdeplot, \"values\",fill=True,cbar=True)\n",
    "\n",
    "#g = sns.catplot(data=ks_df,col=\"metric\",x=\"scenario\",y=\"values\",kind=\"box\",aspect=2,sharey=False,col_order=[\"p-value\",\"statistic\"])\n",
    "g = sns.catplot(data=ks_df,col=\"metric\",x=\"scenario\",y=\"values\",kind=\"box\",aspect=2,sharey=False,col_order=[\"p-value\",\"statistic\"], palette=\"tab10\")\n",
    "\n",
    "#g.add_legend(loc=\"upper right\")\n",
    "g.set_xlabels(\"\")\n",
    "g.set_xticklabels([\"18 days\",\"24 days\",\"29 days\"])\n",
    "ax = g.axes\n",
    "\n",
    "\n",
    "#print(ax)\n",
    "#ax[0].legend(loc='upper center', bbox_to_anchor=(1, -0.20),fancybox=True, shadow=True, ncol=5, labels=[\"18 days\",\"24 days\", \"29 days\"])\n",
    "\n",
    "plt.savefig(\"images/KS-user-distribution.png\", dpi  = 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contacts\n",
    "end_dates = [\"2009-05-25 00:00:00\", \"2009-06-05 00:00:00\"]\n",
    "sns.set(style=\"ticks\",font_scale=3)\n",
    "scale = 800\n",
    "detour_radius = 100\n",
    "ks_df = pd.DataFrame(columns=[\"KS\",\"p-value\",\"uid\",\"scenario\"])\n",
    "start = datetime.datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "for end in end_dates:\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    end =  datetime.datetime.strptime(end, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "    contacts_path = out_path+\"swap/\"+start+\"_\"+end+\"/contacts_\"+str(scale)+\"_\"+str(detour_radius)+\".csv\"\n",
    "    contacts = pd.read_csv(contacts_path).pivot(\"node_a\", \"node_b\", \"contacts\")\n",
    "    contacts=contacts.fillna(0)\n",
    "    print(contacts.sum().sum())\n",
    "    g = sns.heatmap(data=contacts,linewidth=.5,cmap=\"crest\",square=True, vmin=0,vmax=700,cbar_kws={\"shrink\": .82})\n",
    "    plt.xlabel(r'node ID')\n",
    "    plt.ylabel(r'node ID')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"images/user-contacts_\"+str(end)+\".png\", dpi  = 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745ae84c",
   "metadata": {},
   "source": [
    "## Analysis of the Distributed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47df559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.2)  \n",
    "sns.set_style(\"ticks\")\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "pois_filtered = pd.read_csv(path_pois)  \n",
    "\n",
    "start = \"2009-05-01\"\n",
    "end = \"2009-05-15\"\n",
    "scales = [50,100,500,800]\n",
    "d_radius = [100,200,500,800]\n",
    "pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\n",
    "\n",
    "dist_coveraged = pd.DataFrame()\n",
    "for scale in scales:\n",
    "    for detour_radius in d_radius:\n",
    "        d = load_distributed(scale, detour_radius, start, end, False)\n",
    "        dist_coveraged = pd.concat([dist_coveraged,d])\n",
    "\n",
    "        \n",
    "g = sns.FacetGrid(dist_coveraged, row=\"scale\", col=\"detour_radius\",despine = True, aspect = 2)\n",
    "g.map(sns.kdeplot, \"probability\",bw_adjust=.5, clip_on=True,fill=True,color = \"green\", alpha=0.6, linewidth=1.5)\n",
    "\n",
    "\n",
    "# # passing color=None to refline() uses the hue mapping\n",
    "#g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n",
    "g.set_xlabels('$P(C^h)$')\n",
    "g.savefig('./images/distributed_analysis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c93f03",
   "metadata": {},
   "source": [
    "## Plot Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e52078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_filtered = pd.read_csv(path_pois)  \n",
    "H = len(pois_filtered)\n",
    "start = datetime.datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "\n",
    "scales = [50]\n",
    "\n",
    "for scale in scales:\n",
    "    for end in end_date:\n",
    "        end =  datetime.datetime.strptime(end, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "        centralized_coverage = load_centralized(scale, start, end)\n",
    "        plot_model(centralized_coverage,H, \"centralized_\"+str(scale)+\"_\"+str(end))\n",
    "        \n",
    "for scale in scales:\n",
    "    for end in end_date:\n",
    "        end =  datetime.datetime.strptime(end, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "        for detour_radius in d_radius:\n",
    "            distributed_coverage = load_distributed(scale,detour_radius,start,end,False)\n",
    "            plot_model(distributed_coverage,H, \"distributed_\"+str(scale)+\"_\"+str(detour_radius)+\"_\"+str(end))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa570f0",
   "metadata": {},
   "source": [
    "# Load the coverage output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d3ad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "series_dir = run_path+\"cov_series/\"\n",
    "series_files = os.listdir(series_dir)\n",
    "\n",
    "#geo_filtered = pd.read_csv('../../../datasets/mobility/Geolife_Trajectories_1.3/Data/geolife_full_augmented.csv')\n",
    "pois_filtered = pd.read_csv(path_pois)  \n",
    "cov = pd.read_csv(run_path+\"coverage_\"+str(scale)+\"_\"+str(detour_radius)+\".csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "image_dst = run_path+\"images/\"\n",
    "Path(image_dst).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "cov_series = []\n",
    "if '.ipynb_checkpoints' in series_files:\n",
    "    series_files.remove('.ipynb_checkpoints')\n",
    "#print(series_files)\n",
    "\n",
    "for file in series_files:\n",
    "    #print(series_dir + file)\n",
    "    df = pd.read_csv(series_dir + file).rename(columns={\"Unnamed: 0\": \"id_location\"})\n",
    "    node = file.split(\"_\")[0]\n",
    "    df['node'] = node\n",
    "    cov_series.append(df)\n",
    "    \n",
    "cov_series = pd.concat(cov_series)\n",
    "#harmonize dtype\n",
    "#cov_series[['id_location']] = cov_series[['id_location']].astype(int)\n",
    "#pois_filtered[['id_location']] = pois_filtered[['id_location']].astype(int)\n",
    "#df1 = cov_series[cov_series.isna().any(axis=1)]\n",
    "#print(\"df1\",df1)\n",
    "cov_series = cov_series.join(pois_filtered.set_index('id_location'), on=\"id_location\", how=\"left\")\n",
    "cov_series = cov_series.dropna()  #added Alex 17/10\n",
    "#df2 = cov_series[cov_series.isna().any(axis=1)]\n",
    "#print(\"df2\",df2)\n",
    "#print('{5.2f}'.format(df2.id_location))\n",
    "\n",
    "cov = cov.join(pois_filtered.set_index('id_location'), on=\"id_location\", how=\"left\")\n",
    "cov = cov.dropna()\n",
    "\n",
    "locations = pd.read_csv(path_pois)\n",
    "H = len(locations)\n",
    "print(\"Locations found \", H)\n",
    "\n",
    "###### Load the distributed coverage\n",
    "distributed_coverage = {'id_location':[], 'probability':[]}\n",
    "# compute the joint probability for each of the POIs\n",
    "\n",
    "for poi in cov.id_location.unique():\n",
    "    \n",
    "    probs = cov[cov.id_location == poi]['probability']\n",
    "    product = 1\n",
    "    \n",
    "    for p in probs:\n",
    "        product *= (1-float(p))\n",
    "    product = 1 - product    \n",
    "    distributed_coverage['id_location'].append(poi)\n",
    "    distributed_coverage['probability'].append(product)\n",
    "\n",
    "    \n",
    "distributed_coverage = pd.DataFrame.from_dict(distributed_coverage).join(pois_filtered.set_index('id_location'), on=\"id_location\", how=\"left\")\n",
    "\n",
    "\n",
    "##### Load the centralized coverage\n",
    "centralized_coverage = pd.read_csv(centralized_model_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a92bee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Coverage series examination\n",
    "\n",
    "we are going to observe the variation during the time of a chosen node the coverage map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7d930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = '1'\n",
    "node_serie = cov_series[cov_series.node == node]\n",
    "\n",
    "numcols, numrows = 100, 100\n",
    "print(image_dst)\n",
    "Path(f\"{image_dst}/series/{node}/3D/\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"{image_dst}/series/{node}/heatmap/\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "count = 1 \n",
    "for c in node_serie.copy_id.unique():\n",
    "    data = node_serie[node_serie.copy_id == c]\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    \n",
    "    xi = np.linspace(data.lon.min(), data.lon.max(), numrows)\n",
    "    yi = np.linspace(data.lat.min(), data.lat.max(), numcols)\n",
    "\n",
    "    #print(xi, yi)\n",
    "\n",
    "    xi, yi = np.meshgrid(xi, yi)\n",
    "\n",
    "\n",
    "    x, y, z = data.lon, data.lat, data.probability\n",
    "\n",
    "\n",
    "    # known points (long, lat)\n",
    "    points = np.vstack((x,y)).T \n",
    "    # known z points\n",
    "    values = z\n",
    "    # all the points I want to interpolate\n",
    "    wanted = (xi, yi)\n",
    "    # meshgrid interpolated\n",
    "    zi = griddata(points, values, wanted,method=\"linear\", fill_value = 0) \n",
    "\n",
    "    surf = ax.plot_surface(xi, yi, zi, cmap=cm.Reds,\n",
    "                           linewidth=0, antialiased=False,shade=True)\n",
    "\n",
    "    ax.set_zlim(0, 1)\n",
    "    #ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    #ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "    # Add a color bar which maps values to colors.\n",
    "    #fig.colorbar(surf, shrink=0.3, aspect=5)\n",
    "    \n",
    "    bid = node_serie[node_serie.copy_id == c].bid.unique()[0]\n",
    "    print(bid)\n",
    "    #plt.show()\n",
    "    plt.ioff()\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(7, 7)\n",
    "    plt.title(f'{bid}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{img_dest}/series/{node}/3D/{count}_{node}_3D_grid.png', dpi  = 100)\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    \n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(7, 7)\n",
    "    plt.contourf(xi, yi, zi)\n",
    "    plt.scatter(data.lon, data.lat, c=data.probability, s=60, vmin=zi.min(), vmax=zi.max(),cmap=cm.Reds)\n",
    "    plt.ticklabel_format(useOffset=False)\n",
    "    plt.colorbar()\n",
    "    plt.title(f'{bid}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{img_dest}/series/{node}/heatmap/{count}_{node}_heatmap.png\", dpi  = 300)\n",
    "    \n",
    "    plt.close()\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf93c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images = [imageio.imread(f\"{img_dest}/series/{node}/heatmap/{i + 1}_{node}_heatmap.png\") for i in range(len(os.listdir(f'{img_dest}/series/{node}/heatmap/')) - 2)]\n",
    "#kwrgs = { 'duration': 0.5 }\n",
    "#imageio.mimsave(f'{img_dest}/{node}_heatmap.gif', images, **kwrgs)\n",
    "#images = [imageio.imread(f\"{img_dest}/series/{node}/3D/{i + 1}_{node}_3D_grid.png\") for i in range(len(os.listdir(f'{img_dest}/series/{node}/3D/')) - 2)]\n",
    "#kwrgs = { 'duration': 0.5 }\n",
    "#imageio.mimsave(f'{img_dest}/{node}_3D.gif', images, **kwrgs)\n",
    "node = '128'\n",
    "writer = imageio.get_writer(f\"{img_dest}/{node}_heatmap.mp4\", fps=3)\n",
    "for i in range(len(os.listdir(f'{img_dest}/series/{node}/heatmap/')) - 2):\n",
    "    im = imageio.imread(f\"{img_dest}/series/{node}/heatmap/{i + 1}_{node}_heatmap.png\")\n",
    "    writer.append_data(im)\n",
    "writer.close()\n",
    "\n",
    "writer = imageio.get_writer(f\"{img_dest}/{node}_3D_grid.mp4\", fps=3)\n",
    "for i in range(len(os.listdir(f'{img_dest}/series/{node}/3D/')) - 2):\n",
    "    im = imageio.imread(f\"{img_dest}/series/{node}/3D/{i + 1}_{node}_3D_grid.png\")\n",
    "    writer.append_data(im)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351766a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Time-lapse of the coverage map\n",
    "![Gif](images/exp_2/128.gif \"node 17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf098cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Final coverages examination\n",
    "\n",
    "plot a 3d graphic for each of the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3949fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverages = []\n",
    "fused_coveragemcols, numrows = 100, 100\n",
    "\n",
    "coverages = []\n",
    "\n",
    "for n in cov.uid.unique():\n",
    "    data = cov[cov.uid == n]\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    \n",
    "    xi = np.linspace(data.lon.min(), data.lon.max(), numrows)\n",
    "    yi = np.linspace(data.lat.min(), data.lat.max(), numcols)\n",
    "\n",
    "    #print(xi, yi)\n",
    "\n",
    "    xi, yi = np.meshgrid(xi, yi)\n",
    "\n",
    "\n",
    "    x, y, z = data.lon, data.lat, data.probability\n",
    "\n",
    "\n",
    "    # known points (long, lat)\n",
    "    points = np.vstack((x,y)).T \n",
    "    # known z points\n",
    "    values = z\n",
    "    # all the points I want to interpolate\n",
    "    wanted = (xi, yi)\n",
    "    # meshgrid interpolated\n",
    "    zi = griddata(points, values, wanted,method=\"linear\", fill_value = 0) \n",
    "\n",
    "    surf = ax.plot_surface(xi, yi, zi, cmap=cm.Reds,\n",
    "                           linewidth=0, antialiased=False,shade=True)\n",
    "\n",
    "    ax.set_zlim(0, 1)\n",
    "    #ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    #ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "    # Add a color bar which maps values to colors.\n",
    "    #fig.colorbar(surf, shrink=0.3, aspect=5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(7, 7)\n",
    "    plt.title(f'Coverage map of node {n}')\n",
    "    Path(f\"{img_dest}/nodes/\").mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(f'{img_dest}/nodes/{n}_final_3D_grid.png', dpi  = 100)\n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb75cdfa",
   "metadata": {},
   "source": [
    "### Distributed vs Centralized Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca29300e",
   "metadata": {},
   "source": [
    "Distributed coverage maps fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numcols, numrows = 100, 100\n",
    "\n",
    "dist = plt.figure(figsize=(15,15))\n",
    "ax = dist.gca(projection='3d')\n",
    "\n",
    "xi = np.linspace(distributed_coverage.lon.min(), distributed_coverage.lon.max(), numrows)\n",
    "yi = np.linspace(distributed_coverage.lat.min(), distributed_coverage.lat.max(), numcols)\n",
    "\n",
    "#print(xi, yi)\n",
    "\n",
    "xi, yi = np.meshgrid(xi, yi)\n",
    "\n",
    "\n",
    "x, y, z = distributed_coverage.lon, distributed_coverage.lat, distributed_coverage.probability/H\n",
    "\n",
    "\n",
    "# known points (long, lat)\n",
    "points = np.vstack((x,y)).T \n",
    "# known z points\n",
    "values = z\n",
    "# all the points I want to interpolate\n",
    "wanted = (xi, yi)\n",
    "# meshgrid interpolated\n",
    "zi = griddata(points, values, wanted,method=\"linear\", fill_value = 0) \n",
    "\n",
    "surf = ax.plot_surface(xi, yi, zi, cmap=cm.viridis,\n",
    "                       linewidth=0, antialiased=False,shade=True)\n",
    "ax.set_xlabel(r'Longitude', fontsize=15, rotation=60)\n",
    "ax.set_ylabel('Latitude', fontsize=15, rotation=60)\n",
    "ax.set_zlabel('Density', fontsize=15, rotation=60)\n",
    "\n",
    "\n",
    "ax.set_zlim(0, distributed_coverage.probability.max()/H)\n",
    "#ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "#ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "#fig.colorbar(surf, shrink=0.3, aspect=5)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(image_dst+\"/distributed_final_3D_grid.png\", dpi  = 150)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 7)\n",
    "plt.contourf(xi, yi, zi)\n",
    "#plt.scatter(distributed_coverage.lon, distributed_coverage.lat, c=distributed_coverage.probability, s=60, vmin=zi.min(), vmax=zi.max(),cmap=cm.Reds)\n",
    "plt.ticklabel_format(useOffset=False)\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "#plt.savefig(image_dst+\"/distributed_final_heatmap.png\", dpi  = 150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc3bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#centralized = centralized[centralized.lon <= 116.411544]\n",
    "numcols, numrows = 100, 100\n",
    "\n",
    "\n",
    "dist = plt.figure(figsize=(15,15))\n",
    "ax = dist.gca(projection='3d')\n",
    "\n",
    "xi = np.linspace(centralized_coverage.lon.min(), centralized_coverage.lon.max(), numrows)\n",
    "yi = np.linspace(centralized_coverage.lat.min(), centralized_coverage.lat.max(), numcols)\n",
    "\n",
    "#print(xi, yi)\n",
    "\n",
    "xi, yi = np.meshgrid(xi, yi)\n",
    "\n",
    "\n",
    "x, y, z = centralized_coverage.lon, centralized_coverage.lat, centralized_coverage.probability/H\n",
    "\n",
    "# known points (long, lat)\n",
    "points = np.vstack((x,y)).T \n",
    "# known z points\n",
    "values = z\n",
    "# all the points I want to interpolate\n",
    "wanted = (xi, yi)\n",
    "# meshgrid interpolated\n",
    "zi = griddata(points, values, wanted,method=\"linear\", fill_value = 0) \n",
    "\n",
    "surf = ax.plot_surface(xi, yi, zi, cmap=cm.viridis,\n",
    "                       linewidth=0, antialiased=False,shade=True)\n",
    "ax.set_xlabel(r'Longitude', fontsize=15, rotation=60)\n",
    "ax.set_ylabel('Latitude', fontsize=15, rotation=60)\n",
    "ax.set_zlabel('Density', fontsize=15, rotation=60)\n",
    "\n",
    "ax.set_zlim(0, distributed_coverage.probability.max()/H)\n",
    "#ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "#ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "#fig.colorbar(surf, shrink=0.3, aspect=5)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(image_dst+\"/centralized_final_3D_grid.png\", dpi  = 150)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 7)\n",
    "plt.contourf(xi, yi, zi)\n",
    "# plt.scatter(centralized_coverage.lon, centralized_coverage.lat, c=centralized_coverage.probability, s=60, vmin=zi.min(), vmax=zi.max(),cmap=cm.Reds)\n",
    "plt.ticklabel_format(useOffset=False)\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "#plt.savefig(image_dst+\"/centralized_final_heatmap.png\", dpi  = 150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92becab8",
   "metadata": {},
   "source": [
    "## KS test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ks_2samp(distributed_coverage.sort_values(by=\"id_location\")['probability'], centralized_coverage.join(distributed_coverage.set_index('id_location'), on=\"id_location\", how=\"right\", rsuffix=\"_distr\").sort_values(by=\"id_location\")['probability'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f53b8",
   "metadata": {},
   "source": [
    "# Coverage probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b110268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "sns.histplot(data=centralized_coverage, x=\"probability\", color = \"g\", alpha = 0.6)\n",
    "plt.xlabel(\"Coverage probability\")\n",
    "plt.ylabel(\"\")\n",
    "savefig(\"cov_dist_scenario1.png\", dpi  = 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
