{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef78994",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4089337f-f419-4fe9-bd7a-65e411ac74cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install geopy\n",
    "#! pip install csv\n",
    "#! pip install itertools\n",
    "#!pip install random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6343fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import scipy.integrate as integrate\n",
    "from scipy.stats import expon\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from geopy.distance import distance\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import itertools as it\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "from scipy.spatial.distance import cityblock\n",
    "import pickle\n",
    "\n",
    "\n",
    "import math\n",
    "global conf\n",
    "ROOT = Path().resolve()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3945a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(s):\n",
    "    try:\n",
    "        return float(s)\n",
    "    except ValueError:\n",
    "        num, denom = s.split('/')\n",
    "        return float(num) / float(denom)\n",
    "\n",
    "def append_to_csv(file_path, data):\n",
    "    dir = os.path.dirname(file_path)\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    if not os.path.exists(file_path):\n",
    "        # If the file doesn't exist, create a new file\n",
    "        data.to_csv(file_path, index=True)\n",
    "    else:\n",
    "        # If the file exists, append the data to it\n",
    "        data.to_csv(file_path, index=True, mode='a', header=False)\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    # total_meters = METERS * c\n",
    "    r = 6371000  # radiu * 1000 to return meters\n",
    "    return c * r\n",
    "\n",
    "\n",
    "\n",
    "def calculate_coverage(min_distance, scale, detour_radius):\n",
    "    # initialize coverage inner function\n",
    "    #return integrate.quad(lambda x: expon.pdf(x, scale=scale), min_distance, np.inf)[0]\n",
    "\n",
    "    ## Truncate of the Exponential Distribution\n",
    "    return integrate.quad(lambda x: expon.pdf(x, scale=scale), min_distance, np.inf)[0]/(1-math.exp(-scale*detour_radius))\n",
    "\n",
    "# clean all the files inside the run path\n",
    "def clean_run_path(run_path):\n",
    "    if(Path(run_path).exists()):\n",
    "        shutil.rmtree(run_path)\n",
    "        Path(run_path).mkdir(parents=True, exist_ok=True)\n",
    "        Path(run_path+\"cov_series\").mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        Path(run_path).mkdir(parents=True, exist_ok=True)\n",
    "        Path(run_path+\"cov_series\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#MANFREDO calculate euclidean distance and manhattan distance between coverage map final and node's coverage\n",
    "def euclidean_distance(covmap):\n",
    "    df_final = pd.read_csv(path_covmap)\n",
    "    probability_final = np.array(df_final.sort_values(by=\"id_location\")['probability'])\n",
    "    cov_to_test = np.array(covmap.sort_index(axis=0)[\"probability\"])\n",
    "    \n",
    "    return np.linalg.norm(probability_final - cov_to_test)\n",
    "\n",
    "def manhattan_distance(covmap):\n",
    "    df_final = pd.read_csv(path_covmap)\n",
    "    probability_final = np.array(df_final.sort_values(by=\"id_location\")['probability'])\n",
    "    cov_to_test = np.array(covmap.sort_index(axis=0)[\"probability\"])\n",
    "\n",
    "    return cityblock(probability_final,cov_to_test)\n",
    "\n",
    "class Node:\n",
    "\n",
    "    def __init__(self, uid, pois, scale, detour_radius, ageing):\n",
    "        self.uid = uid\n",
    "        self.POIs = pois\n",
    "        self.scale = scale\n",
    "        self.ageing = ageing\n",
    "        self.detour_radius = detour_radius\n",
    "        #self.log = []\n",
    "        #self.merge_log = []  # Keeps track of the data exchange with other nodes\n",
    "        self.cov_count = 0\n",
    "        self.coverage = pd.DataFrame(columns=[\"probability\"])\n",
    "        #self.coverage_map = pd.read_csv(\"swap_covmap_real2.csv\")\n",
    "        self.coverage_map = pd.read_csv(path_covmap)\n",
    "        #self.backup_coverages = []\n",
    "        self.current_trajectory = pd.DataFrame(columns=[\"lat\", \"lon\", \"tid\"])\n",
    "        self.adjacency = {}\n",
    "        self.recent_exchanges = []\n",
    "        self.ageing_start = {}\n",
    "        self.black_list = {}\n",
    "        self.filter_threshold = 0\n",
    "\n",
    "    def move(self, lat, lon, tid, bid):\n",
    "        if self.current_trajectory.size != 0:\n",
    "\n",
    "            if self.current_trajectory.loc[0].tid != tid:\n",
    "                #print(\"--> Computing node coverage map\")\n",
    "                start_coverage = time.perf_counter()\n",
    "                self.compute_coverage_map(bid)\n",
    "                end_coverage = time.perf_counter()\n",
    "                #self.log.append((self.uid, end_coverage - start_coverage))\n",
    "                #print(f\"--> Coverage map computed in {end_coverage - start_coverage:0.4f}\")\n",
    "\n",
    "                # forse togliere\n",
    "                # self.trajectories.append(self.current_trajectory)\n",
    "\n",
    "                self.current_trajectory = pd.DataFrame(columns=[\"lat\", \"lon\", \"tid\"])\n",
    "\n",
    "            self.current_trajectory.loc[len(self.current_trajectory)] = [lat, lon, tid]\n",
    "\n",
    "        else:\n",
    "            self.current_trajectory.loc[0] = [lat, lon, tid]\n",
    "\n",
    "    def compute_coverage_map(self, bid):\n",
    "        # Michele : aggiunto check\n",
    "        if self.ageing:\n",
    "            self.do_age_coverage(bid)\n",
    "\n",
    "        for index, location in self.POIs.iterrows():\n",
    "\n",
    "            d = min(\n",
    "                haversine(location.lat,\n",
    "                                   location.lon,\n",
    "                                   self.current_trajectory['lat'].values,\n",
    "                                   self.current_trajectory['lon'].values\n",
    "                                   )\n",
    "            )\n",
    "\n",
    "            #attack PASL\n",
    "            \n",
    "            if self.uid not in attackers:\n",
    "                    #coverage_probability = 0.0\n",
    "                    #coverage_probability = random.random()\n",
    "                if d <= self.detour_radius:\n",
    "                    coverage_probability = calculate_coverage(d, self.scale, self.detour_radius)\n",
    "                else:\n",
    "                    coverage_probability = 0.0\n",
    "            else:\n",
    "                coverage_probability = 0.0\n",
    "            \n",
    "            \n",
    "            #random attack\n",
    "            '''\n",
    "            if self.uid not in attackers:\n",
    "                if d <= self.detour_radius:\n",
    "                    coverage_probability = calculate_coverage(d, self.scale, self.detour_radius)\n",
    "                else:\n",
    "                    coverage_probability = 0.0\n",
    "            else:\n",
    "                coverage_probability = random.random()\n",
    "            '''    \n",
    "            \n",
    "            if location.id_location in self.coverage.index:\n",
    "                #print(\"probability_union\", self.coverage.loc[location.id_location][0], coverage_probability)\n",
    "                if self.uid not in attackers:\n",
    "                    self.coverage.loc[location.id_location] = [probability_union(self.coverage.loc[location.id_location][0], coverage_probability)]\n",
    "                else:\n",
    "                    self.coverage.loc[location.id_location] = coverage_probability   #attack PASL commentate queste due\n",
    "            else:\n",
    "                coverage_to_add = pd.DataFrame(coverage_probability, columns=[\"probability\"], \n",
    "                                         index=[location.id_location])\n",
    "                self.coverage = pd.concat([self.coverage, coverage_to_add])\n",
    "\n",
    "            #print(\"Coverage for location {} and node {} calculated successfully with value: {} and distance {}\".format(\n",
    "                #location.id_location,\n",
    "                #self.uid,\n",
    "                #self.coverage.loc[location.id_location].probability,\n",
    "                #d)\n",
    "            #)\n",
    "\n",
    "            if location.id_location in self.ageing_start.keys():\n",
    "                if coverage_probability > 0:\n",
    "                    self.ageing_start[index] = bid\n",
    "            else:\n",
    "                self.ageing_start[location.id_location] = bid\n",
    "\n",
    "        # SAVE COPY OF EVERY COVERAGE MAP\n",
    "        cov_copy = self.coverage.copy()\n",
    "        cov_copy['copy_id'] = self.cov_count\n",
    "        cov_copy['bid'] = bid\n",
    "\n",
    "        append_to_csv(run_path+\"cov_series/\"+str(self.uid)+\"_coverages.csv\", cov_copy)\n",
    "        #self.backup_coverages.append(cov_copy)\n",
    "        self.cov_count += 1\n",
    "\n",
    "        # RESTORE RECENT EXCHANGES LIST\n",
    "        self.recent_exchanges = []\n",
    "\n",
    "        #inizialize filter threshold\n",
    "        #if self.filter_threshold == 0:\n",
    "        self.filter_threshold = manhattan_distance(self.coverage)\n",
    "        #self.filter_threshold = euclidean_distance(self.coverage)\n",
    "        #check if there are some users in blacklist with a lower manhattan distance then the new threshold\n",
    "        keys_to_remove = []\n",
    "        for user in self.black_list.keys():\n",
    "            if self.black_list[user] <= self.filter_threshold:\n",
    "                print(\"NEW THRESHOLD after COMPUTE COVMAP, RIMOZIONE DALLA BL DI:\",user)\n",
    "                keys_to_remove.append(user)\n",
    "        for node in keys_to_remove:\n",
    "            del self.black_list[node]\n",
    "                \n",
    "\n",
    "    def do_age_coverage(self, bid):\n",
    "\n",
    "        for location in self.coverage.index:\n",
    "\n",
    "            if self.coverage.loc[location][0] != 0:\n",
    "                ageing_percentage = power_law(start=self.ageing_start[location], current=bid)\n",
    "\n",
    "                if ageing_percentage < 1:\n",
    "                    self.ageing_start[location] = bid\n",
    "                    self.coverage.loc[location] = self.coverage.loc[location] * ageing_percentage\n",
    "    '''\n",
    "    def merge_coverage(self, bid):\n",
    "\n",
    "        if self.ageing:\n",
    "            self.do_age_coverage(bid)\n",
    "\n",
    "        for node in self.adjacency.keys():\n",
    "\n",
    "            if node not in self.recent_exchanges:\n",
    "\n",
    "                # merge current coverage map with all the nodes of the adjacency list and remove current node from the in o\n",
    "                # order to ovid multiple, useless, merge operations\n",
    "                if self.ageing:\n",
    "                    self.adjacency[node].do_age_coverage(bid)\n",
    "\n",
    "                cov, ageing = merge(self.coverage, self.adjacency[node].coverage, self.ageing_start,\n",
    "                                          self.adjacency[node].ageing_start)\n",
    "                self.coverage = cov.copy()\n",
    "                self.adjacency[node].coverage = cov\n",
    "                self.ageing_start = copy.deepcopy(ageing)\n",
    "                self.adjacency[node].ageing_start = ageing\n",
    "\n",
    "                # Since at every time bucket the position of a node is the result of an average of the points of its trajectory,\n",
    "                # it could be possible that a node x has another node y in his adjacency list but not vice versa\n",
    "                try:\n",
    "                    self.adjacency[node].recent_exchanges.append(self.uid)\n",
    "                    del self.adjacency[node].adjacency[self.uid]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                finally:\n",
    "                    self.recent_exchanges.append(node)\n",
    "                    self.merge_log.append((self.uid, node, bid))\n",
    "\n",
    "        self.adjacency = {}\n",
    "'''        \n",
    "                  \n",
    "\n",
    "def probability_union(p1, p2):\n",
    "    # P(AUB) = P(A) + P(B) - P(A)*P(B)\n",
    "    return 1 - (1 - p1) * (1 - p2)\n",
    "\n",
    "'''\n",
    "def merge(cov1, cov2, ageing1, ageing2):\n",
    "    for index, row in cov1.iterrows():\n",
    "\n",
    "        if index in cov2.index:\n",
    "\n",
    "            cov2.at[index] = probability_union(row.probability, cov2.at[index, 'probability'])\n",
    "            ageing2[index] = max(ageing2[index], ageing1[index])\n",
    "\n",
    "        else:\n",
    "            cov2 = cov2.append(pd.DataFrame(row.probability, columns=[\"probability\"],\n",
    "                                            index=[index]))\n",
    "            ageing2[index] = ageing1[index]\n",
    "\n",
    "    return cov2, ageing2\n",
    "'''\n",
    "\n",
    "def power_law(start, current):\n",
    "    days = int(current - start) / 1000000000 / 60 / 60 / 24\n",
    "\n",
    "    if days == 0:\n",
    "        return 1\n",
    "\n",
    "    res = conf['powerlaw_a_param'] * pow(days, conf['powerlaw_k_param'])\n",
    "\n",
    "    if res >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return res\n",
    "        \n",
    "#filter with manhattan distance\n",
    "\n",
    "def filter_check(node_a_uid, node_b_uid, manhattan_a, manhattan_b):\n",
    "    if (manhattan_a <= nodes[node_b_uid].filter_threshold ) and (manhattan_b <= nodes[node_a_uid].filter_threshold):\n",
    "        return True\n",
    "    #check if node_a blacklist is full, check if the new Manhattan is the lowest in the blacklist, in that case return TRUE \n",
    "    elif (manhattan_b > nodes[node_a_uid].filter_threshold) and (manhattan_a <= nodes[node_b_uid].filter_threshold):\n",
    "        if len(nodes[node_a_uid].black_list) == max_blacklist:\n",
    "            #print(\"FILTER CHECK NODE A: \", node_a_uid)\n",
    "            check = True\n",
    "            for key in nodes[node_a_uid].black_list.keys():\n",
    "                if nodes[node_a_uid].black_list[key] < manhattan_b:\n",
    "                    check = False\n",
    "            if check == True:\n",
    "                nodes[node_a_uid].filter_threshold = manhattan_b\n",
    "                return True\n",
    "    #check if node_b blacklist is full, check if the new Manhattan is the lowest in the blacklist, in that case return TRUE \n",
    "    elif (manhattan_a > nodes[node_b_uid].filter_threshold) and (manhattan_b <= nodes[node_a_uid].filter_threshold):\n",
    "        if len(nodes[node_b_uid].black_list) == max_blacklist:\n",
    "            #print(\"FILTER CHECK NODE B: \", node_b_uid)\n",
    "            check = True\n",
    "            for key in nodes[node_b_uid].black_list.keys():\n",
    "                if nodes[node_b_uid].black_list[key] < manhattan_a:\n",
    "                    check = False\n",
    "            if check == True:\n",
    "                nodes[node_b_uid].filter_threshold = manhattan_a\n",
    "                return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#filter with euclidean distance\n",
    "'''\n",
    "def filter_check(node_a_uid, node_b_uid, euclidean_a, euclidean_b):\n",
    "    #if nodes[node_b_uid].filter_threshold == 0 or nodes[node_a_uid].filter_threshold == 0:\n",
    "    #    return True\n",
    "    #if (node_b_uid in nodes[node_a_uid].black_list.keys()) or (node_a_uid in nodes[node_b_uid].black_list.keys()):\n",
    "    #    return False\n",
    "    if (euclidean_a <= nodes[node_b_uid].filter_threshold ) and (euclidean_b <= nodes[node_a_uid].filter_threshold):\n",
    "        return True\n",
    "    #check if node_a blacklist is (max - 1), check if the new euclidean is the lowest in the blacklist, in that case return TRUE \n",
    "    elif (euclidean_b > nodes[node_a_uid].filter_threshold) and (euclidean_a <= nodes[node_b_uid].filter_threshold):\n",
    "        if len(nodes[node_a_uid].black_list) == max_blacklist-1:\n",
    "            #print(\"FILTER CHECK NODE A: \", node_a_uid)\n",
    "            check = True\n",
    "            for key in nodes[node_a_uid].black_list.keys():\n",
    "                if nodes[node_a_uid].black_list[key] < euclidean_b:\n",
    "                    check = False\n",
    "            if check == True:\n",
    "                nodes[node_a_uid].filter_threshold = euclidean_b\n",
    "                return True\n",
    "    #check if node_b blacklist is (max - 1), check if the new euclidean is the lower in the blacklist, in that case return TRUE \n",
    "    elif (euclidean_a > nodes[node_b_uid].filter_threshold) and (euclidean_b <= nodes[node_a_uid].filter_threshold):\n",
    "        if len(nodes[node_b_uid].black_list) == max_blacklist-1:\n",
    "            #print(\"FILTER CHECK NODE B: \", node_b_uid)\n",
    "            check = True\n",
    "            for key in nodes[node_b_uid].black_list.keys():\n",
    "                if nodes[node_b_uid].black_list[key] < euclidean_a:\n",
    "                    check = False\n",
    "            if check == True:\n",
    "                nodes[node_b_uid].filter_threshold = euclidean_a\n",
    "                return True\n",
    "    else:\n",
    "        return False\n",
    "'''\n",
    "#MANFREDO attack 2, attackers receive a covmap with a swap and change probability value from this one\n",
    "def poisoning_data(covmap):\n",
    "    for index, row in covmap.iterrows():\n",
    "        if row.probability > 0.5:\n",
    "            #print(str(index)+\" prima: \"+str(row.probability)+\" --> dopo: 0.01\")\n",
    "            covmap.at[index,'probability'] = 0.01\n",
    "        elif row.probability > 0:\n",
    "            #print(str(index)+\" prima: \"+str(row.probability)+\" --> dopo: 0.99\")\n",
    "            covmap.at[index,'probability'] = 0.99\n",
    "    return covmap\n",
    "\n",
    "def check_threshold(uid, blacklist):\n",
    "    min_manhattan = 1000\n",
    "    #min_euclidean = 50\n",
    "    keys = blacklist.keys()\n",
    "    keys_to_remove = []\n",
    "    print(\"CHECK THRESHOLD NODE: \",uid)\n",
    "    print(\"NODE IN BL: \", keys)\n",
    "    print(\"OLD THRESHOLD: \", nodes[uid].filter_threshold)\n",
    "    for key in keys:\n",
    "        if blacklist[key] < min_manhattan:\n",
    "        #if blacklist[key] < min_euclidean:    \n",
    "            min_manhattan = blacklist[key]\n",
    "            #min_euclidean = blacklist[key]\n",
    "    nodes[uid].filter_threshold = min_manhattan\n",
    "    #nodes[uid].filter_threshold = min_euclidean\n",
    "    for k in keys:\n",
    "        #if blacklist[k] == min_euclidean:\n",
    "        if blacklist[k] == min_manhattan:\n",
    "            keys_to_remove.append(k)\n",
    "    print(\"RIMOZIONI DALLA BL DEI NODI: \", keys_to_remove)\n",
    "    for node in keys_to_remove:\n",
    "        del nodes[uid].black_list[node]\n",
    "    \n",
    "    print(\"NEW THRESHOLD: \", min_manhattan)\n",
    "\n",
    "#final clean of all blacklists before sending them to the server\n",
    "def clean_blacklists():\n",
    "    for node in nodes.keys():\n",
    "        keys = nodes[node].black_list.keys()\n",
    "        keys_to_remove = []\n",
    "        for key in keys:\n",
    "            if nodes[node].black_list[key] <= nodes[node].filter_threshold:\n",
    "                keys_to_remove.append(key)\n",
    "        for uid in keys_to_remove:\n",
    "            del nodes[node].black_list[uid]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8632c9a6",
   "metadata": {},
   "source": [
    "## Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f7d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "nodes = {}\n",
    "time_buckets = []\n",
    "\n",
    "\n",
    "\n",
    "def initialize(path_gps, contacts, granularity=None, start_date=None, end_date=None, synthetic=False):\n",
    "    print(\"Initialization started...\")\n",
    "    print(\"Reading:\",path_gps)\n",
    "    geo = pd.read_csv(path_gps, parse_dates = True)\n",
    "\n",
    "    print(\"-> Filtering geolife\")\n",
    "\n",
    "    geo['date_time'] = pd.to_datetime(geo['date_time'], errors='coerce')\n",
    "\n",
    "    # fiilter with the dates\n",
    "    geo = geo[(geo.date_time >= start_date) & (geo.date_time <= end_date)]\n",
    "\n",
    "    # filter with the bounding box\n",
    "    geo = geo[(geo['lat'].between(beijing_lat_min, beijing_lat_max)) & (geo['lon'].between(beijing_lon_min, beijing_lon_max))]\n",
    "    \n",
    "    print(len(geo))\n",
    "    \n",
    "    \n",
    "    if granularity == \"min\":\n",
    "        geo['date_time'] = geo['date_time'].dt.floor('min')\n",
    "    if granularity == \"h\":\n",
    "        geo['date_time'] = geo['date_time'].dt.floor('h')\n",
    "    if granularity == \"sec\":\n",
    "        geo['date_time'] = geo['date_time'].dt.floor('s')\n",
    "\n",
    "    users = geo['uid'].unique()\n",
    "    max_blacklist = np.round((percentage_attackers*len(users))/100)\n",
    "    print(\"MAX BLACKLIST: \", max_blacklist)\n",
    "    \n",
    "    geo = geo.groupby(['date_time', 'uid'], as_index=False).mean()\n",
    "\n",
    "    for time in geo['date_time'].unique():\n",
    "        time_buckets.append(geo[geo['date_time'] == time])\n",
    "\n",
    "    # create contacts dataframe\n",
    "    print(\"Creating contacts file\")\n",
    "    uids = set(geo[\"uid\"])\n",
    "    dfs_to_add = [\n",
    "        pd.DataFrame({\"node_a\": [node_a], \"node_b\": [node_b], \"contacts\": [0]})\n",
    "        for node_a, node_b in it.combinations(uids, 2)\n",
    "    ]\n",
    "    # keep it in RAM\n",
    "    contacts = pd.concat(dfs_to_add, ignore_index=True)\n",
    "    #contacts.to_csv(run_path+\"contacts\"+\"_\"+str(scale)+\"_\"+str(detour_radius)+\".csv\", index=False)\n",
    "        \n",
    "    print(\"Initialization ended:\",len(time_buckets))\n",
    "    return contacts, max_blacklist\n",
    "\n",
    "def start_simulation(path_pois, scale, detour_radius, contacts, run_path):\n",
    "    print(\"Reading POIs...\")\n",
    "    pois = pd.read_csv(path_pois)\n",
    "    #MANFREDO create res to save swaps and statistics for each swap\n",
    "    res=[]\n",
    "    filter=[]\n",
    "    \n",
    "    #filter POIs according to the Beijing\n",
    "    print(\"number of locations:\",len(pois))\n",
    "    \n",
    "    counter = 1\n",
    "    for bucket in time_buckets:\n",
    "        #print(bucket[\"date_time\"].unique())\n",
    "        #start_bucket = time.perf_counter()\n",
    "\n",
    "        inner_counter = 1\n",
    "        for c_index, current_node in bucket.iterrows():\n",
    "            #print(\"--> Processing node {} of {}\".format(inner_counter, len(bucket)))\n",
    "            inner_counter += 1\n",
    "            #print(str(inner_counter))\n",
    "            if current_node.uid not in nodes.keys():\n",
    "                ageing = conf[\"ageing_active\"]\n",
    "                print(\"CREAZIONE NODO: \", current_node.uid)\n",
    "                nodes[current_node.uid] = Node(current_node.uid, pois, scale, detour_radius, ageing)\n",
    "                nodes[current_node.uid].move(lat=current_node.lat, lon=current_node.lon, tid=current_node.tid,\n",
    "                                             bid=bucket['date_time'].values[0])\n",
    "            else:\n",
    "                nodes[current_node.uid].move(lat=current_node.lat, lon=current_node.lon, tid=current_node.tid,bid=bucket['date_time'].values[0])\n",
    "\n",
    "            if conf[\"ADJACENCY_THRESHOLD\"] > 0:\n",
    "                for node in nodes:\n",
    "                    if current_node.uid == node: continue\n",
    "                    node_b = node\n",
    "                    #idx = contacts.index[(contacts[\"node_a\"]==node_a) & (contacts[\"node_b\"] == node_b)]\n",
    "                    #r = contacts.loc[idx]\n",
    "                    #if r.empty:\n",
    "                        #contacts = pd.concat([contacts, pd.Series([node_a,node_b,0])], ignore_index=True)\n",
    "                    #    contacts = contacts.append({\"node_a\":node_a,\"node_b\":node_b,\"contacts\":0},ignore_index=True)\n",
    "                    #print(\"--> Computing nodes adjacency\")\n",
    "                    #start_adjacency = time.perf_counter()\n",
    "\n",
    "                    # Calculate distance between two nodes\n",
    "                    d = distance((float(current_node.lat),\n",
    "                                  float(current_node.lon)),\n",
    "                                 (float(nodes[node_b].current_trajectory.tail(1).lat.iloc[0]),\n",
    "                                  float(nodes[node_b].current_trajectory.tail(1).lon.iloc[0])))\n",
    "                    \n",
    "                    if d.meters <= conf['ADJACENCY_THRESHOLD']:\n",
    "                        #idx = contacts.index[(contacts[\"node_a\"]==node_a) & (contacts[\"node_b\"] == node_b)]\n",
    "                        #r = contacts.loc[idx]\n",
    "                        #contacts.loc[idx,['contacts']] = r[\"contacts\"]+1\n",
    "                        #print(\"contact:\",current_node.uid,nodes[neighbor])\n",
    "                        filtered_rows = contacts[\n",
    "                        ((contacts[\"node_a\"] == current_node.uid) & (contacts[\"node_b\"] == node_b)) |\n",
    "                        ((contacts[\"node_a\"] == node_b) & (contacts[\"node_b\"] == current_node.uid))\n",
    "                        ]\n",
    "                        contacts.loc[filtered_rows.index, \"contacts\"] += 1                   \n",
    "\n",
    "                        #MANFREDO count number of POI with non zero probability\n",
    "                        node_a_POI = np.count_nonzero(nodes[current_node.uid].coverage)\n",
    "                        node_b_POI = np.count_nonzero(nodes[node_b].coverage)\n",
    "                        \n",
    "                        #calculate euclidean and manhattan distance between coverage and final coverage map\n",
    "                        if node_a_POI > 0:\n",
    "                            euclidean_a = euclidean_distance(nodes[current_node.uid].coverage)\n",
    "                            manhattan_a = manhattan_distance(nodes[current_node.uid].coverage)\n",
    "                        else:\n",
    "                            euclidean_a = 0\n",
    "                            manhattan_a = 0\n",
    "                        if node_b_POI > 0:\n",
    "                            euclidean_b = euclidean_distance(nodes[node_b].coverage)\n",
    "                            manhattan_b = manhattan_distance(nodes[node_b].coverage)\n",
    "                        else:\n",
    "                            euclidean_b = 0\n",
    "                            manhattan_b = 0\n",
    "                        \n",
    "                        res.append([int(current_node.uid), int(node_b), node_a_POI, node_b_POI, euclidean_a, euclidean_b, manhattan_a, manhattan_b,nodes[current_node.uid].filter_threshold,nodes[node_b].filter_threshold])\n",
    "                        # filtro per accettare lo scambio\n",
    "                        if filter_active == True:\n",
    "                            if filter_check(current_node.uid, node_b, manhattan_a, manhattan_b) == True:\n",
    "                            #if (filter_check(current_node.uid, node_b, euclidean_a, euclidean_b) == True):\n",
    "                                #caso scambio valido\n",
    "                                #print(\"SWAP EFFETTUATO\")\n",
    "                                #if euclidean_a == 0 and euclidean_b==0:\n",
    "                                if manhattan_a == 0 and manhattan_b==0:\n",
    "                                    filter.append([int(current_node.uid), int(node_b), 1, 1])\n",
    "                                elif (current_node.uid in attackers and node_a_POI > 0) or (node_b in attackers and node_b_POI > 0):\n",
    "                                    filter.append([int(current_node.uid), int(node_b), 1, 0])\n",
    "                                else:\n",
    "                                    filter.append([int(current_node.uid), int(node_b), 1, 1])\n",
    "                                #nodes[current_node.uid].adjacency[neighbor] = nodes[neighbor]\n",
    "                                old_coverage = nodes[current_node.uid].coverage.copy()\n",
    "                                \n",
    "                                #attackers don't take the other's covmap (random attack)\n",
    "                                #if current_node.uid not in attackers:\n",
    "                                #    nodes[current_node.uid].coverage = nodes[node_b].coverage.copy()\n",
    "                                #if node_b not in attackers:\n",
    "                                #    nodes[node_b].coverage = old_coverage\n",
    "\n",
    "                                #attack2.0\n",
    "                                nodes[current_node.uid].coverage = nodes[node_b].coverage.copy()\n",
    "                                nodes[node_b].coverage = old_coverage\n",
    "                                \n",
    "                                #initialize threshold, if are 0, with coverage map received , if not 0, threshold take the manhattan value from the coverage map received\n",
    "                                #euclidean\n",
    "                                #if nodes[current_node.uid].filter_threshold == 0 and euclidean_b !=0 :\n",
    "                                #    nodes[current_node.uid].filter_threshold = euclidean_distance(nodes[current_node.uid].coverage) \n",
    "                                #if nodes[node_b].filter_threshold == 0 and euclidean_a != 0:\n",
    "                                #    nodes[node_b].filter_threshold = euclidean_distance(nodes[node_b].coverage)\n",
    "                                #manhattan    \n",
    "                                if nodes[current_node.uid].filter_threshold == 0 and manhattan_b !=0 :\n",
    "                                    nodes[current_node.uid].filter_threshold = manhattan_distance(nodes[current_node.uid].coverage) \n",
    "                                if nodes[node_b].filter_threshold == 0 and manhattan_a != 0:\n",
    "                                    nodes[node_b].filter_threshold = manhattan_distance(nodes[node_b].coverage)\n",
    "                                \n",
    "                                \n",
    "                                #attack 2.0\n",
    "                                if node_b in attackers:\n",
    "                                    nodes[node_b].coverage = poisoning_data(nodes[node_b].coverage)\n",
    "                                elif current_node.uid in attackers:\n",
    "                                    nodes[current_node.uid].coverage = poisoning_data(nodes[current_node.uid].coverage)\n",
    "                            else:\n",
    "                                #print(\"SWAP NON EFFETTUATO\")\n",
    "                                if (current_node.uid in attackers) or (node_b in attackers):\n",
    "                                    filter.append([int(current_node.uid), int(node_b), 0, 0])\n",
    "                                else:\n",
    "                                    filter.append([int(current_node.uid), int(node_b), 0, 1])\n",
    "                                #blacklist append here\n",
    "                                #if (euclidean_a != 0 and euclidean_a > nodes[node_b].filter_threshold):\n",
    "                                if (manhattan_a != 0 and manhattan_a > nodes[node_b].filter_threshold):\n",
    "                                    #node_a probably is an attacker, append node_a in node_b's black list\n",
    "                                    if current_node.uid not in nodes[node_b].black_list:\n",
    "                                        #nodes[node_b].black_list[int(current_node.uid)] = euclidean_a\n",
    "                                        nodes[node_b].black_list[int(current_node.uid)] = manhattan_a\n",
    "                                        if len(nodes[node_b].black_list) == max_blacklist:\n",
    "                                            check_threshold(node_b,nodes[node_b].black_list)     #aggiornamento threshold e rimozione utente con distanza minore\n",
    "                                #elif (euclidean_b != 0 and euclidean_b > nodes[current_node.uid].filter_threshold):\n",
    "                                elif (manhattan_b != 0 and manhattan_b > nodes[current_node.uid].filter_threshold):\n",
    "                                    #node_b probably is an attacker, append node_b in node_a's black list\n",
    "                                    if node_b not in nodes[current_node.uid].black_list:\n",
    "                                        #nodes[current_node.uid].black_list[int(node_b)] = euclidean_b\n",
    "                                        nodes[current_node.uid].black_list[int(node_b)] = manhattan_b\n",
    "                                        if len(nodes[current_node.uid].black_list) == max_blacklist:\n",
    "                                            check_threshold(current_node.uid,nodes[current_node.uid].black_list) #aggiornamento threshold e rimozione utente con distanza minore\n",
    "                                   \n",
    "                                #print(\"after\",nodes[node_a].coverage.head()) \n",
    "                        else:\n",
    "                            #filter not active, swap\n",
    "                            old_coverage = nodes[current_node.uid].coverage.copy()   \n",
    "                            nodes[current_node.uid].coverage = nodes[node_b].coverage.copy()\n",
    "                            nodes[node_b].coverage = old_coverage\n",
    "                            #attack PASL without filter\n",
    "                            '''\n",
    "                            if node_b in attackers:\n",
    "                                #print(\"ATTACKER: \"+str(node_b))\n",
    "                                nodes[node_b].coverage = poisoning_data(nodes[node_b].coverage)\n",
    "                            if current_node.uid in attackers:\n",
    "                                #print(\"ATTACKER: \"+str(current_node.uid))\n",
    "                                nodes[current_node.uid].coverage = poisoning_data(nodes[current_node.uid].coverage)\n",
    "                           '''\n",
    "\n",
    "                #end_adjacency = time.perf_counter()\n",
    "                #print(f\"--> Node adjacency computed in {end_adjacency - start_adjacency:0.4f}\")\n",
    "                #log.append((bucket['date_time'].values[0], \"adjacency\", end_adjacency - start_adjacency))\n",
    "\n",
    "        \n",
    "        #end_bucket = time.perf_counter()\n",
    "        #print(\"-> Bucket analyzed in {}\".format(end_bucket - start_bucket))\n",
    "\n",
    "        #print(\"-> Merging coverages map\") if len(nodes.keys()) > 0 else print(\"-> No merge\")\n",
    "        #start_merging = time.perf_counter()\n",
    "#         for node in nodes.keys():\n",
    "#             if nodes[node].adjacency != {}:\n",
    "#                 print(\"node:\",nodes[node].uid)\n",
    "#                 nodes[node].swap_coverage(bid=bucket['date_time'].values[0])\n",
    "        #end_merging = time.perf_counter()\n",
    "        #log.append((bucket['date_time'].values[0], \"merge\", end_merging - start_merging))\n",
    "        #print(f\"-> Merge procedure completed in {end_merging - start_merging:0.4f}\\n\")\n",
    "        counter += 1\n",
    "    #MANFREDO saving statistics about swaps and filter\n",
    "    if conf[\"ADJACENCY_THRESHOLD\"] > 0:\n",
    "        df_POI_covered = pd.DataFrame(res,columns=['node_a','node_b','POI_node_a','POI_node_b', 'ED_a', 'ED_b', 'MH_a', 'MH_b', 'Threshold_a', 'Threshold_b'])\n",
    "        df_POI_covered.to_csv(run_path+\"swaps_\"+str(scale)+\"_\"+str(detour_radius)+\".csv\", index=True)\n",
    "        df_filter = pd.DataFrame(filter, columns=['node_a','node_b','filter','true_label'])\n",
    "        df_filter.to_csv(run_path+\"filter_\"+str(scale)+\"_\"+str(detour_radius)+\".csv\", index=True)\n",
    "    #return contacts\n",
    "\n",
    "\n",
    "#def finalize_simulation(run_path):\n",
    "#    bid = time_buckets[-1]['date_time'].values[0]\n",
    "#    for node in nodes.keys():\n",
    "#        nodes[node].compute_coverage_map(bid=bid)\n",
    "        #print(\"nodes raw\",nodes[node].coverage.probability.values)\n",
    "    #for node in nodes.keys():\n",
    "    #    start_merging = time.perf_counter()\n",
    "    #    nodes[node].merge_coverage(bid=bid)\n",
    "    #    end_merging = time.perf_counter()\n",
    "    #    log.append((\"\", \"merge\", end_merging - start_merging))\n",
    "    #    print(\"merged\",nodes[node].coverage.probability.values) \n",
    "#        pd.concat(nodes[node].backup_coverages).to_csv(run_path+\"cov_series/\"+str(node)+\"_coverages.csv\")\n",
    "\n",
    "\n",
    "def save_results(run_path, scale, detour_radius,contacts):\n",
    "    print(\"Collecting coverages...\")\n",
    "    res = []\n",
    "    #logs = []\n",
    "    #merge_logs = []\n",
    "    for node in nodes.keys():\n",
    "        print(\"SALVATAGGIO COVERAGE NODO: \", node)\n",
    "        df_node_with_uid = pd.DataFrame()\n",
    "        df_node_with_uid['uid'] = [node] * len(nodes[node].coverage)\n",
    "        df_node_with_uid['id_location'] = nodes[node].coverage.index\n",
    "        df_node_with_uid['probability'] = nodes[node].coverage.probability.values\n",
    "        df_node_with_uid['scale'] = scale\n",
    "        df_node_with_uid['detour_radius'] = detour_radius\n",
    "        \n",
    "        res.append(df_node_with_uid)\n",
    "        print(\"COVERAGE MAP: \",nodes[node].coverage)\n",
    "        #print(\"RES: \",res)\n",
    "     #   logs += nodes[node].log\n",
    "     #   merge_logs += nodes[node].merge_log\n",
    "\n",
    "    print(\"Saving results...\")\n",
    "    pd.concat(res).to_csv(run_path+\"coverage_\"+str(scale)+\"_\"+str(detour_radius)+\".csv\")\n",
    "    # Print the contact list only if in swap mode\n",
    "    if ADJACENCY_THRESHOLD > 0:\n",
    "        print(contacts)\n",
    "        contacts.to_csv(run_path+\"contacts_\"+str(scale)+\"_\"+str(detour_radius)+\".csv\")\n",
    "        #create dictionary for blacklists and save it in a file\n",
    "        dict_blacklist = {}\n",
    "        clean_blacklists() #clean blacklist before submit to the server\n",
    "        for node in nodes:\n",
    "            node_blacklist = []\n",
    "            for key in nodes[node].black_list.keys():\n",
    "                node_blacklist.append(key)\n",
    "            dict_blacklist[node] = node_blacklist\n",
    "        with open(run_path+\"blacklists_\"+str(scale)+\"_\"+str(detour_radius)+\".pk1\", \"wb\") as fp:\n",
    "            pickle.dump(dict_blacklist, fp)\n",
    "            print('blacklist saved successfully to file')\n",
    "            print(dict_blacklist)\n",
    "#     with open(run_path+'log_coverage.csv', 'w') as out:\n",
    "#         csv_out = csv.writer(out)\n",
    "#         csv_out.writerow(['node', 'time'])\n",
    "#         for row in logs:\n",
    "#             csv_out.writerow(row)\n",
    "\n",
    "#     with open(run_path+\"log_merge.csv\", 'w') as out:\n",
    "#         csv_out = csv.writer(out)\n",
    "#         csv_out.writerow(['node_1', 'node_2', 'bid'])\n",
    "#         for row in merge_logs:\n",
    "#             csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be42bfa",
   "metadata": {},
   "source": [
    "## Execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876ea67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(f\"{ROOT}/conf.yaml\") as f:\n",
    "    conf = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "cov_series_path = conf[\"cov_series_path\"]\n",
    "start_date=conf['start_date']\n",
    "ends =conf['end_date']\n",
    "path_gps = conf['path_gps']\n",
    "out_path = conf[\"out_path\"]\n",
    "path_pois = conf[\"path_pois\"]\n",
    "granularity=conf['granularity']\n",
    "scales = conf[\"scale_values\"]\n",
    "detour_rs = conf[\"detour_radius\"]\n",
    "beijing_lat_min = conf[\"beijing_lat_min\"]\n",
    "beijing_lat_max = conf[\"beijing_lat_max\"]\n",
    "beijing_lon_min = conf[\"beijing_lon_min\"]\n",
    "beijing_lon_max = conf[\"beijing_lon_max\"]\n",
    "ADJACENCY_THRESHOLD = conf[\"ADJACENCY_THRESHOLD\"]\n",
    "filter_active = conf[\"filter_active\"]\n",
    "#max_blacklist = conf[\"max_blacklist\"]\n",
    "percentage_attackers = conf[\"perc_max_attackers\"]\n",
    "attack_path = conf[\"attack_path\"]\n",
    "path_covmap = conf[\"path_covmap\"]\n",
    "print(conf)\n",
    "\n",
    "max_blacklist = 0\n",
    "attackers = [128,30,140,84]\n",
    "#attackers = [30,128]\n",
    "#attackers = []\n",
    "for end_date in ends:\n",
    "    print(\"Period:\",end_date)\n",
    "    if(int(ADJACENCY_THRESHOLD)>0):\n",
    "        out_path+=\"swap/no_filter/\"\n",
    "\n",
    "    if not attackers:\n",
    "        run_path = out_path+datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")+\"_\"+datetime.strptime(end_date, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")+\"/\"\n",
    "    else:\n",
    "        run_path = attack_path+datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")+\"_\"+datetime.strptime(end_date, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")+\"/\"\n",
    "    clean_run_path(run_path)\n",
    "    for scale in scales:\n",
    "        for detour_radius in detour_rs:\n",
    "            contacts = pd.DataFrame(columns=[\"node_a\",\"node_b\",\"contacts\"])\n",
    "            print(\"Simulating with:\",scale,detour_radius)\n",
    "            print(\"Number of attackers: \", len(attackers))\n",
    "            ## MICHELE: PARAMETRIZZARE SCALE; D_RADIUS\n",
    "            #initialize(path_gps,granularity,start_date, end_date)\n",
    "            #contacts = start_simulation(path_pois, scale, detour_radius,contacts)\n",
    "            #save_results(run_path, scale,detour_radius,contacts)\n",
    "            contacts, max_blacklist = initialize(path_gps, contacts, granularity,start_date, end_date)\n",
    "            max_blacklist = max_blacklist +1\n",
    "            print(\"contacts passed\")\n",
    "            print(\"MAX BLACKLIST: \", max_blacklist)\n",
    "            start_simulation(path_pois, scale, detour_radius,contacts, run_path)\n",
    "            print(\"start_simulation passed\")\n",
    "            save_results(run_path, scale,detour_radius,contacts)\n",
    "            print(\"save_results passed\")\n",
    "            \n",
    "#     with open(run_path+\"log_engine.csv\", 'w') as out:\n",
    "#         csv_out = csv.writer(out)\n",
    "#         csv_out.writerow(['bucket', 'type', 'time'])\n",
    "#         for row in log:\n",
    "#             csv_out.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0424b60a-ff9f-49ce-b807-7a4c9b38fc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#controllare se il numero di contatti totali Ã¨ uguale alla dimensione del df number_POI_swap\n",
    "#statistiche swaps tra solo utenti onesti e attaccanti\n",
    "df_contacts = pd.read_csv(\"../../output/distributed_model/test/swap/2009-05-16_2009-05-23/contacts_100_800.csv\")\n",
    "df_number_POI_swap = pd.read_csv(\"../../output/distributed_model/test/swap/attack2/2009-05-16_2009-05-23/swaps_100_800.csv\")\n",
    "len_POI_swap = len(df_number_POI_swap)\n",
    "counter_contacts = 0\n",
    "for index, row in df_contacts.iterrows():\n",
    "    counter_contacts += row.contacts\n",
    "print(\"lunghezza di POI_swap:\", len_POI_swap)\n",
    "print(\"numero totale di contatti:\", counter_contacts)\n",
    "\n",
    "attackers=[128,30]\n",
    "#attackers=[30]\n",
    "\n",
    "a = pd.read_csv(\"../../output/distributed_model/test/swap/2009-05-16_2009-05-23/swaps_100_800.csv\")\n",
    "#for index, row in a.iterrows():\n",
    "#    if (row.len_cov_a == 0 and row.len_cov_b == 0):\n",
    "#        a = a.drop(index)\n",
    "   # elif (row.MH_a == 0 or row.MH_b == 0):\n",
    "   #     a = a.drop(index)\n",
    "mse_a = []\n",
    "mse_b = []\n",
    "mh_a = []\n",
    "ed_a = []\n",
    "mh_b = []\n",
    "ed_b = []\n",
    "for index, row in a.iterrows():\n",
    "    if (row.POI_node_a == 0 and row.POI_node_b == 0):\n",
    "        a = a.drop(index)\n",
    "    elif (row.POI_node_b == 0):\n",
    "        mh_a.append(row.MH_a)\n",
    "        ed_a.append(row.ED_a)\n",
    "        mse_a.append(row.MSE_a)\n",
    "    elif (row.POI_node_a == 0):\n",
    "        mh_b.append(row.MH_b)\n",
    "        ed_b.append(row.ED_b)\n",
    "        mse_b.append(row.MSE_b)\n",
    "    else:\n",
    "        mh_a.append(row.MH_a)\n",
    "        ed_a.append(row.ED_a)\n",
    "        mse_a.append(row.MSE_a)\n",
    "        mh_b.append(row.MH_b)\n",
    "        ed_b.append(row.ED_b)\n",
    "        mse_b.append(row.MSE_b)\n",
    "print(\"REAL describe\")\n",
    "print(\"MH_A:\")\n",
    "print(pd.DataFrame(mh_a).describe())\n",
    "print(\"count mh_b values:\",len(mh_b))\n",
    "print(\"MH_B:\")\n",
    "print(pd.DataFrame(mh_b).describe())\n",
    "print(\"ED_A:\")\n",
    "print(pd.DataFrame(ed_a).describe())\n",
    "print(\"ED_B:\")\n",
    "print(pd.DataFrame(ed_b).describe())\n",
    "print(\"MSE_A:\")\n",
    "print(pd.DataFrame(mse_a).describe())\n",
    "print(\"MSE_B:\")\n",
    "print(pd.DataFrame(mse_b).describe())\n",
    "#print(a.describe())\n",
    "mse_a = []\n",
    "mse_b = []\n",
    "mh_a = []\n",
    "ed_a = []\n",
    "mh_b = []\n",
    "ed_b = []\n",
    "for index, row in df_number_POI_swap.iterrows():\n",
    "    if (row.POI_node_a == 0 and row.POI_node_b == 0):\n",
    "        df_number_POI_swap = df_number_POI_swap.drop(index)\n",
    "    elif (row.node_b not in attackers and row.node_a not in attackers):\n",
    "        df_number_POI_swap = df_number_POI_swap.drop(index)\n",
    "    elif (row.node_a in attackers and row.POI_node_a != 0):\n",
    "        mh_a.append(row.MH_a)\n",
    "        ed_a.append(row.ED_a)\n",
    "        mse_a.append(row.MSE_a)\n",
    "    elif (row.node_b in attackers and row.POI_node_b != 0):\n",
    "        mh_b.append(row.MH_b)\n",
    "        ed_b.append(row.ED_b)\n",
    "        mse_b.append(row.MSE_b)\n",
    "   # elif (row.MH_a == 0 or row.MH_b == 0):\n",
    "   #     df_number_POI_swap = df_number_POI_swap.drop(index)\n",
    "\n",
    "#print(df_number_POI_swap)\n",
    "#b = df_number_POI_swap.describe()\n",
    "print(\"ATTACK describe\")\n",
    "#print(b)\n",
    "\n",
    "print(\"MEDIE DELLA MH SOLO PER GLI ATTACCANTI:\")\n",
    "print(\"count mh_a values:\",len(mh_a))\n",
    "#print(sum(mh_a)/len(mh_a))\n",
    "print(\"MH_A:\")\n",
    "print(pd.DataFrame(mh_a).describe())\n",
    "print(\"count mh_b values:\",len(mh_b))\n",
    "print(\"MH_B:\")\n",
    "print(pd.DataFrame(mh_b).describe())\n",
    "print(\"ED_A:\")\n",
    "print(pd.DataFrame(ed_a).describe())\n",
    "print(\"ED_B:\")\n",
    "print(pd.DataFrame(ed_b).describe())\n",
    "print(\"MSE_A:\")\n",
    "print(pd.DataFrame(mse_a).describe())\n",
    "print(\"MSE_B:\")\n",
    "print(pd.DataFrame(mse_b).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20faa41d-6f2c-41e1-b924-cd711f049ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "#valutazione filtro con confusion matrix e metriche di valutazione\n",
    "\n",
    "with open(f\"{ROOT}/conf.yaml\") as f:\n",
    "    conf = yaml.load(f, Loader=yaml.FullLoader)\n",
    "out_path = conf['out_path']\n",
    "#start_date=conf['start_date']\n",
    "attack_path = conf['attack_path']\n",
    "\n",
    "#filter_real = pd.read_csv(out_path+\"2009-05-01_2009-05-07/filter_100_800.csv\")\n",
    "#filter_attack = pd.read_csv(out_path+\"attack/2009-05-01_2009-05-15/filter_100_800.csv\")\n",
    "#print(filter_attack)\n",
    "out_path = out_path+\"swap/\"\n",
    "#start = datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "start = \"2009-05-24\"\n",
    "#end_date = [\"2009-05-07 00:00:00\", \"2009-05-15 00:00:00\"]\n",
    "#end_date = [\"2009-05-15 00:00:00\"]\n",
    "#end_date = [\"2009-05-23 00:00:00\"]\n",
    "end_date = [\"2009-05-31 00:00:00\"]\n",
    "for end in end_date:\n",
    "    end =  datetime.strptime(end, '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")\n",
    "    print(end)\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    print(attack_path+start+\"_\"+end+\"/filter_100_800.csv\")\n",
    "    #print(path_covmap)\n",
    "    #filter_attack = pd.read_csv(out_path+start+\"_\"+end+\"/filter_100_800.csv\")\n",
    "    filter_attack = pd.read_csv(attack_path+start+\"_\"+end+\"/filter_100_800.csv\")\n",
    "    for index, row in filter_attack.iterrows():\n",
    "       # print(row.true_label)\n",
    "        if (row['true_label'] == 1) and (row['filter'] == 1):\n",
    "            true_positive +=1\n",
    "        elif (row['true_label']) == 0 and (row['filter'] ==0):\n",
    "            true_negative +=1\n",
    "        elif (row['true_label'] == 1) and (row['filter'] ==0):\n",
    "            false_negative +=1\n",
    "        else:\n",
    "            false_positive +=1\n",
    "    \n",
    "    print(\"true positive: \",true_positive)\n",
    "    print(\"false positive: \",false_positive)\n",
    "    print(\"true negative: \",true_negative)\n",
    "    print(\"false negative: \",false_negative)\n",
    "    \n",
    "    confusion_matrix_real = confusion_matrix(filter_attack['true_label'],filter_attack['filter'])\n",
    "    display = ConfusionMatrixDisplay(confusion_matrix_real)\n",
    "    display.plot()\n",
    "    plt.show()\n",
    "    accuracy = accuracy_score(filter_attack['true_label'],filter_attack['filter'])\n",
    "    precision = (true_positive/(true_positive + false_positive))\n",
    "    recall = (true_positive/(true_positive + false_negative))\n",
    "    f1 = f1_score(filter_attack['true_label'],filter_attack['filter'])\n",
    "    print(\"accuracy score: \",accuracy)\n",
    "    print(\"precision: \", precision)\n",
    "    print(\"recall: \", recall)\n",
    "    print(\"f1 score:\", f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1488c0-8042-4a9d-9989-166e11f09c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset aumentato analisi\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "global conf\n",
    "ROOT = Path().resolve()\n",
    "\n",
    "with open(f\"{ROOT}/conf.yaml\") as f:\n",
    "    conf = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "cov_series_path = conf[\"cov_series_path\"]\n",
    "start_date=conf['start_date']\n",
    "ends =conf['end_date']\n",
    "path_gps = conf['path_gps']\n",
    "out_path = conf[\"out_path\"]\n",
    "path_pois = conf[\"path_pois\"]\n",
    "granularity=conf['granularity']\n",
    "beijing_lat_min = conf[\"beijing_lat_min\"]\n",
    "beijing_lat_max = conf[\"beijing_lat_max\"]\n",
    "beijing_lon_min = conf[\"beijing_lon_min\"]\n",
    "beijing_lon_max = conf[\"beijing_lon_max\"]\n",
    "\n",
    "for end_date in ends:\n",
    "    '''\n",
    "    print(\"Initialization started...\")\n",
    "    print(\"Reading: ../../../dataset/geolife_full_augmented.csv\")\n",
    "    geo = pd.read_csv(\"../../../dataset/geolife_full_augmented.csv\", parse_dates = True)\n",
    "    \n",
    "    print(\"-> Filtering geolife augmented\")\n",
    "    \n",
    "    geo['date_time'] = pd.to_datetime(geo['date_time'], errors='coerce')\n",
    "    \n",
    "    # fiilter with the dates\n",
    "    geo = geo[(geo.date_time >= start_date) & (geo.date_time <= end_date)]\n",
    "    \n",
    "    # filter with the bounding box\n",
    "    geo = geo[(geo['lat'].between(beijing_lat_min, beijing_lat_max)) & (geo['lon'].between(beijing_lon_min, beijing_lon_max))]\n",
    "    \n",
    "    print(\"LUNGHEZZA GEO AUGMENTED: \",len(geo))\n",
    "\n",
    "    if granularity == \"min\":\n",
    "        geo['date_time'] = geo['date_time'].dt.floor('min')\n",
    "    if granularity == \"h\":\n",
    "        geo['date_time'] = geo['date_time'].dt.floor('h')\n",
    "    if granularity == \"sec\":\n",
    "        geo['date_time'] = geo['date_time'].dt.floor('s')\n",
    "    \n",
    "    users = geo['uid'].unique()\n",
    "    print(\"ALL USERS: \", users)\n",
    "    print(\"NUMBER USERS: \", len(users))\n",
    "    for user in users:\n",
    "        print(\"user : \", user)\n",
    "        print(\"count: \", len(geo[geo['uid']==user]))\n",
    "    '''\n",
    "    print(\"start_date: \", start_date)\n",
    "    print(\"end_date: \", end_date)\n",
    "    print(\"Reading: ../../../dataset/geo_life_full.csv\")\n",
    "    geo = pd.read_csv(\"../../../dataset/geo_life_full.csv\", parse_dates = True)\n",
    "    \n",
    "    print(\"-> Filtering geolife\")\n",
    "    \n",
    "    geo['date_time'] = pd.to_datetime(geo['date_time'], errors='coerce')\n",
    "    \n",
    "    # fiilter with the dates\n",
    "    geo = geo[(geo.date_time >= start_date) & (geo.date_time <= end_date)]\n",
    "    \n",
    "    # filter with the bounding box\n",
    "    geo = geo[(geo['lat'].between(beijing_lat_min, beijing_lat_max)) & (geo['lon'].between(beijing_lon_min, beijing_lon_max))]\n",
    "    \n",
    "    print(\"LUNGHEZZA GEO: \",len(geo))\n",
    "    \n",
    "    \n",
    "    if granularity == \"min\":\n",
    "        geo['date_time'] = geo['date_time'].dt.floor('min')\n",
    "    if granularity == \"h\":\n",
    "        geo['date_time'] = geo['date_time'].dt.floor('h')\n",
    "    if granularity == \"sec\":\n",
    "        geo['date_time'] = geo['date_time'].dt.floor('s')\n",
    "    \n",
    "    users = geo['uid'].unique()\n",
    "    print(\"ALL USERS: \", users)\n",
    "    print(\"NUMBER USERS: \", len(users))\n",
    "    for user in users:\n",
    "        print(\"user : \", user)\n",
    "        print(\"count: \", len(geo[geo['uid']==user]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb0de6-9f27-4b24-afb8-965764ab4882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
